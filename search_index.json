[["index.html", "Bayes Rules! Book Club Welcome", " Bayes Rules! Book Club The R4DS Online Learning Community 2022-08-04 Welcome Welcome to the bookclub! This is a companion for the book Bayes Rules! by Alicia A. Johnson, Miles Q. Ott, and Mine Dogucu (Chapman and Hall/CRC, copyright 2022, 9780367255398). This companion is available at r4ds.io/bayes_rules. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We‚Äôll try to cover 1 chapter/week, but‚Ä¶ ‚Ä¶It‚Äôs ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. Following the flow! Source: https://www.youtube.com/watch?v=zYYBtxHWE0A From: Richard McElreath, Statistical Rethinking videos "],["preface.html", "Preface ", " Preface "],["bayesian-statistics.html", "0.1 Bayesian statistics?", " 0.1 Bayesian statistics? Frequentist and Bayesian methods share: learning from data But Bayesian allows: new data + prior results easier to interpret shines when frequentist fails computational tools more accesible now "],["tips-and-tricks-from-the-authors.html", "0.2 Tips and tricks from the authors", " 0.2 Tips and tricks from the authors Learn by doing Embrace a growth mindset (we will do mistakes!) Interpret Bayes in a context (ethics and maybe more) Practice, practice, practice "],["set-up.html", "0.3 Set up", " 0.3 Set up Install rstan : https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started install.packages(c(&quot;bayesrules&quot;, &quot;tidyverse&quot;, &quot;janitor&quot;, &quot;rstanarm&quot;, &quot;bayesplot&quot;, &quot;tidybayes&quot;, &quot;broom.mixed&quot;, &quot;modelr&quot;, &quot;e1071&quot;, &quot;forcats&quot;), dependencies = TRUE) On linux (ubuntu 22) I had to update some dependencies. "],["the-authors.html", "0.4 The authors:", " 0.4 The authors: Alicia A. Johnson : Website https://ajohns24.github.io/portfolio/ Miles Q. Ott: https://twitter.com/Miles_Ott Mine Dogucu: https://twitter.com/MineDogucu "],["the-big-bayesian-picture.html", "Chapter 1 The Big (Bayesian) Picture", " Chapter 1 The Big (Bayesian) Picture Learning objectives: Learn to think like a Bayesian. Explore the foundations of a Bayesian data analysis and how they contrast with the frequentist alternative Learn a little bit about the history of the Bayesian philosophy "],["thinking-like-a-bayesian-14.html", "1.1 Thinking like a Bayesian 1/4", " 1.1 Thinking like a Bayesian 1/4 DiagrammeR::grViz(&quot; digraph thinking_bayesian{ # node statement node [shape = oval] a [label = &#39;Prior&#39;]; b [label = &#39;Data&#39;]; c [label = &#39;Posterior&#39;]; d [label = &#39;New data&#39;]; e [label = &#39;Posterior&#39;]; f [label = &#39;New data&#39;] g [style = invisible ] # edge statement a -&gt; c b -&gt; c c -&gt; e d -&gt; e f-&gt; g [style = dashed] e-&gt; g [style = dashed] }&quot;) Figure 1.1: A Bayesian knowledge-building diagram Both Bayesian and frequentist share a common goal: learn from data about the world around Both use data to fit nodels, make predictions and evaluate hypothesis "],["quiz-time.html", "1.2 Quiz time!", " 1.2 Quiz time! 4-5: frequentist 6-8: a bit of both 9-12: Bayesian TODO link to script and data in repo "],["thinking-like-a-bayesian-24.html", "1.3 Thinking like a Bayesian 2/4", " 1.3 Thinking like a Bayesian 2/4 1.3.1 Interpreting probability: Bayesian philosophy: relative plausibility of an event Frequentist philosophy: long-run relative frequency of a repeatable event "],["thinking-like-a-bayesian-34.html", "1.4 Thinking like a Bayesian 3/4", " 1.4 Thinking like a Bayesian 3/4 1.4.1 Bayesian balancing act Two claims: Zuofo claims he can predict the outcome of coin flip Kavya claims she can distinguish between natural and artificial sweeteners If both succeed with a 10/10 sucess rate what can we conclude from this? The frequentist approach will discard prior knowledge (it is harder to predict coin flip that having a sensitive palate to sweeteners) and the Bayesian want to use this prior knowledge. -&gt; How can we balance Prior and Data? "],["thinking-like-a-bayesian-44.html", "1.5 Thinking like a Bayesian 4/4", " 1.5 Thinking like a Bayesian 4/4 1.5.1 Asking question What‚Äôs the chance that I actually have the disease (a)? Versus I do not have the disease, What‚Äôs the chance that I would have gotten this positive test results (b)? # building data disease &lt;- c(rep(&quot;disease&quot;, 4), rep(&quot;no disease&quot;, 96)) a &lt;- &quot;test positive&quot; ; b &lt;- &quot;test negative&quot; test &lt;- c(rep(a, 3), b, rep(a, 9), rep(b, 87)) disease_status &lt;- data.frame(disease, test) # contingency table contingency_disease &lt;- table(disease_status) contingency_disease &lt;- addmargins(contingency_disease) knitr::kable(contingency_disease ) test negative test positive Sum disease 1 3 4 no disease 87 9 96 Sum 88 12 100 (a): 3 / 12 (b): 9 / 96 Analogy between (b) and p-value: it is more natural to study the uncertainty of a yet-unproven hypothesis than the uncertainty of data we have already observed.(authors‚Äôopinion) "],["quick-history-lesson.html", "1.6 Quick history lesson", " 1.6 Quick history lesson From stigmatized to being used in modeling COVID-19 rates. Why? advances in computing departure from tradition (what people learn is what people use) reevaluation of subjectivity : frequentist is also subjective and subjectivity is not any more a dirty word. "],["look-ahead.html", "1.7 Look ahead", " 1.7 Look ahead 1.7.1 4 units Bayesian foundations: 5 chapters Focus: models &amp; distributions (conjugate family) Posterior simulations &amp; analysis: 3 chapters Focus: when conjugate is not an option: MCMC then posterior analysis Bayesian regression &amp; classification Focus: extending unit 1 reponse variable (Y) with predictor variables (X) Hierarchical Bayesian models Focus: expanding unit 3 to accomodate and harness grouped data. "],["summary.html", "1.8 Summary", " 1.8 Summary Posterior knowledge &lt;- balancing information from data and prior knowledge More ‚Äúwaves‚Äùof data -&gt; refine knowledge (less effect of prior) With more and more data, two analysts will converge on the same posterior knowledge "],["resources-mentioned.html", "1.9 Resources mentioned", " 1.9 Resources mentioned This is a list of resources mentioned in the first meeting! 1.9.1 Other Bayesian books: Richard McElreath * book: https://xcelab.net/rm/statistical-rethinking/ * vid√©o: https://github.com/rmcelreath/stat_rethinking_2022 The ‚Äúpuppy‚Äù book by John K. Kruschke : https://sites.google.com/site/doingbayesiandataanalysis/ Introduction to Bayesian Thinking, Clyde, Centinkaya-Rundel et al similar level to our book Bayesian Data Analysis, Andrew Gelman more precise (and mathematical) Intro to Bayes Theorem, Wrath of Math, video Clear explanation of the meaning of given in statements like probability of A given B. $ P(A | B) $ 1.9.2 Drawing DAG (Directed Acyclic Graph) Pen and paper DiagrammeR: for drawing diagram, uses Graphviz or mermaid Dagitty: for causal diagrams 1.9.3 Podcast Learning Bayesian Statistics ep. 42 With Mine Dogucu "],["meeting-videos.html", "1.10 Meeting Videos", " 1.10 Meeting Videos 1.10.1 Cohort 1 Meeting chat log 00:06:01 Olivier: hello ! 00:06:47 Olivier: various links : https://r4ds.github.io/bookclub-bayes_rules/ 00:06:55 Olivier: https://docs.google.com/spreadsheets/d/18IDSOU2bfkD55kOB18qCB7Idbpiyp4_9qeWjkvE-Syc/edit#gid=0 00:09:44 Olivier: Let≈õ wait that everyone configure zoom :P 00:10:18 Gabby Palomo: I didn&#39;t see if anyone else signed up for this cohort. I imagine more people did, right? 00:10:46 Olivier: I do not know the number of participant 00:10:52 Olivier: I will have to ask 00:11:08 Will: Well hopefully. I&#39;m assuming for fellow Brits there are lots people celebrating the jubilee. 00:11:59 Gabby Palomo: ah that&#39;s true!! 00:12:10 erik.aalto@tocaboca.com: I‚Äôm not hearing anything, but I could hear recording in progress when I joined.. 00:12:24 Olivier: do you hear me ? 00:12:27 Gabby Palomo: There are 85 people in the slack channel so will see. 00:12:42 erik.aalto@tocaboca.com: Nope, cant hear anything:/ 00:13:15 Will: Yes I think I can hear Erik 00:13:45 erik.aalto@tocaboca.com: Darn, can‚Äôt hear anything‚Ä¶this is weird 00:13:57 Olivier: we hear you at least 00:14:03 erik.aalto@tocaboca.com: I hear the zoom notifs‚Ä¶like ‚Äùrecording in progress‚Äù 00:15:52 Olivier: it is fine 00:16:13 Olivier: is it better 00:17:24 Olivier: is it good now ? 00:17:41 Olivier: i restaart it 00:19:02 Olivier: working or not ? 00:19:20 Ronald Legere: Not yet‚Ä¶ there is a audio test thing in the audio settings menu 00:19:48 Olivier: good name 00:56:58 Ronald Legere: Can you link those podcasts to slack? Or here ;) 01:06:31 Olivier Leroy: DiagrammeR 1.10.2 Cohort 2 "],["bayes-rule.html", "Chapter 2 Bayes‚Äô Rule", " Chapter 2 Bayes‚Äô Rule Learning objectives: Explore foundational probability tools conditional probability: Probability of A given B \\(P(A|B)\\) joint probability: Probability of A and B occuring \\(P(A \\cap B)\\) marginal probability: Probability of an event \\(P(A)\\) Law of Total Probability: If a probability of an event is unknown it can be calculated using the know probability of other related event Conduct first formal Bayesian analysis Practice your Bayesian grammar Prior Likelihood Normalizing constant Simulate Bayesian models sample() sample_n() rbinon() "],["building-a-bayesian-model-for-events.html", "2.1 Building a Bayesian model for events", " 2.1 Building a Bayesian model for events First Data set ?? fake_news Figure 2.1: Bayesian knowledge-building diagram for wether or not the article is fake Two variables: - fake vs real - ! or not 2.1.1 Workflow: Prior probability model A model for interpreting the data Posterior probability model \\[ P(FakeNew = 0.4 ) \\quad and \\quad P(Real = 0.6) \\] \\[ P(B = 0.4 ) \\quad and \\quad P(B^c = 0.6) \\] Here \\(P(FakeNew)\\) : prior probability of an article to be a fake news Valid probability model : 1. accounts all event 2. assign probabilities for each event 3. sum to one \\(P(ExClam)\\) : probability that an article contains an exclamation mark in his title We know that if an article is fake news : 26.67% that the title contains ! and if it is not fake this is just 2.22%. \\[ P(Exclam|FakeNew) = 0.2667 \\quad and \\quad P(Exclam|Real = 0.0222) \\] This is a conditional probability. Conditional probability help know if B give us insight in A. If it does not provide any information it means that A and B event are independent (\\(P(A|B) = P(A)\\)). "],["normalizing-constant.html", "2.2 Normalizing constant", " 2.2 Normalizing constant \\(P(Exclam|FakeNew) = 0.2667 \\quad and \\quad P(Exclam|Real) = 0.0222)\\) are our likelihood, when we know A (!) we know that getting B (Fake news) is more likely. It is different that our prior probability. Then we need can calculate the joint probability (probability of observing A nd B for example), of each options (here it is half of them): \\[ P(Exclam \\cap FakeNew) = P(Exclam|FakeNew) P (Fakenew) = 0.2667 *0.4 = 0.1067 \\] \\[ P(PasExclam \\cap FakeNew ) = (1 - P(Exclam|FakeNew)) * P(FakeNew)) = (1 - 0.2667) * 0.4 = 0.2993 \\] Here \\(P(B)\\) is the marginal probability of B B &lt;- c(0.1067, 0.2933, 0.4) Bc &lt;- c(0.0133, 0.5867, 0.6) Total &lt;- c(0.12, 0.88, 1) joint_p &lt;- data.frame(B, Bc, Total, row.names = c(&quot;A&quot;, &quot;Ac&quot;, &quot;Total&quot;)) knitr::kable(joint_p) B Bc Total A 0.1067 0.0133 0.12 Ac 0.2933 0.5867 0.88 Total 0.4000 0.6000 1.00 \\(P(Exclam)\\) is our normalizing constant. Ok but we want \\(P(FakeNew|exlam)\\) ie \\(P(B|A)\\) \\[ P(FakeNew|exclam) = \\frac{P(exclam \\cap FakeNew)}{P(Exclam)} = \\frac{P(FakeNew)L(FakeNew|Exclam)}{P(Exclam)} \\] \\[ posterior = \\frac{prior . likelihood}{normalizing \\quad constant} = \\frac{0.4 * 0.2667}{0.12} = 0.889 \\] "],["posterior-simulation.html", "2.3 Posterior simulation", " 2.3 Posterior simulation This was a model. Now we will use a simulation! library(dplyr) library(ggplot2) set.seed(84735) # Define possible articles article &lt;- data.frame(type = c(&quot;real&quot;, &quot;fake&quot;)) # Define the prior model prior &lt;- c(0.6, 0.4) article_sim &lt;- dplyr::sample_n(article, size = 10000, weight = prior, replace = TRUE) # dats model article_sim &lt;- article_sim %&gt;% mutate(data_model = case_when(type == &quot;fake&quot; ~ 0.2667, type == &quot;real&quot; ~ 0.0222)) # Simulate exclamation point usage data &lt;- c(&quot;NoExclam&quot;, &quot;Exclam&quot;) set.seed(3) # Rbase simplier ? article_sim &lt;- article_sim %&gt;% group_by(1:n()) %&gt;% mutate(usage = sample(data, size = 1, prob = c(1 - data_model, data_model))) ggplot(article_sim, aes(x = type)) + geom_bar() + facet_wrap(~ usage) "],["example-pop-vs-soda-vs-coke.html", "2.4 Example Pop vs Soda vs Coke", " 2.4 Example Pop vs Soda vs Coke Expend TRUE/FALSE example with one with categories. "],["building-a-bayesian-model-for-random-variables-1n.html", "2.5 Building a Bayesian model for random variables (1/n)", " 2.5 Building a Bayesian model for random variables (1/n) 2.5.1 First step prior \\(\\pi\\) : skill of Kasparov relative to Deep Blue (random variable) Prior model of \\(\\pi\\): pi &lt;- c(0.2, 0.5, 0.8, &quot;total&quot;) # pmf : probability mass functions pmf &lt;- c(0.1, 0.25, 0.65, 1) prior_pi &lt;- data.frame(pi, pmf) knitr::kable(t(prior_pi)) pi 0.2 0.5 0.8 total pmf 0.10 0.25 0.65 1.00 "],["building-a-bayesian-model-for-random-variables-1n-1.html", "2.6 Building a Bayesian model for random variables (1/n)", " 2.6 Building a Bayesian model for random variables (1/n) 2.6.1 Binomial data model Y is the number of games (on 6 games) that Kasparov wins. Y our random variable: {0, 1, ‚Ä¶., 6} , depends on \\(\\pi\\) \\[f(y|\\pi) = P (Y = y|\\pi ) \\] \\(y\\) : any possible outcone If we assume games are independents (no effect on each other) and \\(\\pi\\) is fixed we can use the Binomial model. Y is the number of successes in a fixed number of trials (\\(n\\)) \\[ Y|\\pi \\sim Bin(n, \\pi) \\] \\[ f(y|\\pi) = \\begin{pmatrix} 6 \\\\y \\end{pmatrix} \\pi^y(1 - \\pi)^{6 - y} \\quad for \\quad y \\in \\begin{Bmatrix} 0, 1, 2, 3, 4, 5, 6 \\end{Bmatrix} \\] We can use the prior for \\(\\pi\\) and all \\(y\\) to calculate each probabilities. "],["building-a-bayesian-model-for-random-variables-1n-2.html", "2.7 Building a Bayesian model for random variables (1/n)", " 2.7 Building a Bayesian model for random variables (1/n) 2.7.1 Binomial likelihood function Kasparov only won one of six games. This is our data with \\(L(\\pi|y = 1)\\). We can calculate it for each \\(\\pi\\) value. What is more likely is that \\(pi\\) was 0.2. 2.7.2 Probability mass functions vs likelihood functions When \\(\\pi\\) is known the conditional pmf allows us to compare the probabilities of different possible value of data Y (y1, y2 ..) occuring with \\(pi\\) when Y = y is known the likelihood function allows us to to compare the relative values of \\(\\pi\\) (\\(\\pi_1, \\pi_2, etc ..\\)) 2.7.3 Normalizing constant Total probability that Kasparov would win Y = 1 game across all possible win probability of \\(\\pi\\) We apply the Law of Total Probability (the sum of all likelihood for each value of \\(\\pi\\) by the prior probailities of these \\(\\pi\\) values) \\[ f(y = 1) = L(\\pi = 0.2 | y = 1) f(\\pi = 0.2 ) + L(\\pi = 0.5 | y = 1) f(\\pi = 0.5 ) + L(\\pi = 0.8 | y = 1) f(\\pi = 0.8 ) +\\] \\[ f( y = 1) \\simeq 0.3932 * 0.1 + 0.0938 * 0.25 + 0.0015 * 0.65 \\simeq 0.0637 \\] 2.7.4 Posterior probability model We have the prior, the likelihod and the normalizing constant -&gt; Bayes Rules! \\[ posterior = \\frac{prior . likelihood}{normalizing \\quad constant} \\quad for \\in \\begin{Bmatrix} 0.2, 0 5, 0.8 \\end{Bmatrix} \\] 2.7.5 Posterior shortcut The normalizing constant, is a constant it appears in all posterior calculations. We can work with unnormalizied proabilities or we can divide each unnormalizied probability by the sum of each of them. \\[ posterior \\propto prior * likelihood \\] ## Posterior simulation # Define possible win probabilities chess &lt;- data.frame(pi = c(0.2, 0.5, 0.8)) # Define the prior model prior &lt;- c(0.10, 0.25, 0.65) # Simulate 10000 values of pi from the prior set.seed(84735) chess_sim &lt;- sample_n(chess, size = 10000, weight = prior, replace = TRUE) chess_sim &lt;- chess_sim %&gt;% mutate(y = rbinom(10000, size = 6, prob = pi)) # Focus on simulations with y = 1 win_one &lt;- chess_sim %&gt;% filter(y == 1) # Plot the posterior approximation ggplot(win_one, aes(x = pi)) + geom_bar() "],["links-shared-in-the-second-meeting.html", "2.8 Links shared in the second meeting", " 2.8 Links shared in the second meeting Bayesian probability for babies! (with cookies) : https://raw.githubusercontent.com/epimath/epid-814-materials/master/Lectures/BayesianEstimation.pdf Author: Marisa Eisenberg "],["meeting-videos-1.html", "2.9 Meeting Videos", " 2.9 Meeting Videos 2.9.1 Cohort 1 Meeting chat log 00:11:44 erik.aalto@tocaboca.com: Hello, sorry for joining late, happy to see you again üòÑ 00:36:06 Brendan Lam: https://www.youtube.com/watch?v=HZGCoVF3YvM 00:37:26 Erik: we‚Äôll utilize the following likelihood function notation &lt;poorly formatted paste&gt; 00:37:41 Erik: sorry for how the formatting above went‚Ä¶ 00:42:42 Lisa: to add to brendan&#39;s resource, here is a slide deck using cookies as an example: https://github.com/epimath/epid-814-materials/blob/master/Lectures/BayesianEstimation.pdf 01:02:00 Federica Gazzelloni: thanks 01:02:34 Federica Gazzelloni: I‚Äôd like to go back to the L(..|..) 01:02:35 Gabby Palomo: It was clear Olivier. Don‚Äôt worry!! 01:03:07 Federica Gazzelloni: just to understand how it works and what is the difference 01:03:43 Erik: Need to jump out! Thanks for today. 01:04:15 Lisa: I need to get back to work too. thank you for today! 01:04:58 Federica Gazzelloni: thanks 01:05:03 Federica Gazzelloni: I‚Äôll check that 2.9.2 Cohort 2 "],["the-beta-binomial-bayesian-model.html", "Chapter 3 The Beta-Binomial Bayesian Model", " Chapter 3 The Beta-Binomial Bayesian Model Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1.html", "3.1 SLIDE 1", " 3.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-2.html", "3.2 Meeting Videos", " 3.2 Meeting Videos 3.2.1 Cohort 1 Meeting chat log 00:10:55 Kasia Ozga: host disabled whitboard so we will not try it üòõ 00:30:25 Will Parbury: 3 Blue 1 Brown: Binomial distributions | Probabilities of probabilities, part 1 https://www.youtube.com/watch?v=8idr1WZ1A7Q Serrano.Academy: The Beta Distribution in 12 minutes! https://www.youtube.com/watch?v=juF3r12nM5A PsychEd: Milgram‚Äôs Obedience Experiment https://www.youtube.com/watch?v=cBDkJ-Nc3Ig 00:30:38 Federica Gazzelloni: thanks 00:35:29 Federica Gazzelloni: you can watch the fist two videos of advancedR cohort6 book club for how to make the notes 00:36:32 Federica Gazzelloni: https://www.youtube.com/playlist?list=PL3x6DOfs2NGjnCxGKeDNJUfPpRFI2hJjv 00:36:58 Lisa: thanks! 00:38:21 Federica Gazzelloni: to make the notes have a look at the first and the third videos 00:38:42 Federica Gazzelloni: just the begin 00:40:39 Kasia Ozga: v 00:40:52 Kasia Ozga: https://ben18785.shinyapps.io/distribution-zoo/ 00:44:06 Kasia Ozga: mean is alpha / (alpha +alpha) 00:44:25 Kasia Ozga: 0,4 = 1/alpha 3.2.2 Cohort 2 "],["balance-and-sequentiality-in-bayesian-analyses.html", "Chapter 4 Balance and Sequentiality in Bayesian Analyses", " Chapter 4 Balance and Sequentiality in Bayesian Analyses Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-1.html", "4.1 SLIDE 1", " 4.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log 00:14:08 Brendan Lam: eventually! 4.2.2 Cohort 2 "],["conjugate-families.html", "Chapter 5 Conjugate Families", " Chapter 5 Conjugate Families Learning objectives: Practice building Bayesian models Familiarize yourself with conjugacy "],["greek-letters.html", "5.1 Greek letters", " 5.1 Greek letters \\(\\lambda\\) = lambda \\(\\mu\\) = mu \\(\\sigma\\) = sigma \\(\\tau\\) = tau \\(\\theta\\) = theta This our last chapter on Bayesian foundations! "],["revisiting-choice-of-prior.html", "5.2 Revisiting choice of prior", " 5.2 Revisiting choice of prior Flexibility Computational ease: posterior easy to build Interpretability 5.2.1 Reminder the Beta-Binomial Model: Prior: \\(Beta (\\alpha, \\beta)\\) Data model: \\(Y = y \\quad for \\quad Bin(n, \\pi)\\) Posterior: \\(Beta(\\alpha + y, \\beta = n - y)\\) "],["joy.html", "5.3 Joy!", " 5.3 Joy! (yes I know it was the other way!) "],["gamma-poisson-conjugate-family-18.html", "5.4 Gamma-Poisson conjugate family 1/8", " 5.4 Gamma-Poisson conjugate family 1/8 We are going to do a model to estimate the number of fraud risk phone call: 5.4.1 Prior: rate \\(\\tau \\approx\\) 5 number of phone call / day can range from 2-7 "],["gamma-poisson-conjugate-family-28.html", "5.5 Gamma-Poisson conjugate family 2/8", " 5.5 Gamma-Poisson conjugate family 2/8 5.5.1 Poisson data model: \\(Y =\\) number of independant event that occur in a fixed ammount of time \\[Y|y \\sim Pois(\\tau) \\] Probability mass function: \\[ f(y|Y) = \\frac{\\tau^ye^-\\tau}{y!} \\quad for y \\in \\{0, 1, 2, ...\\} \\] (sum to 1) \\[E(Y|\\tau) = Var(Y|\\tau) = \\tau \\] "],["gamma-poisson-conjugate-family-38.html", "5.6 Gamma-Poisson conjugate family 3/8", " 5.6 Gamma-Poisson conjugate family 3/8 5.6.1 Poisson pmfs with different \\(\\tau\\) "],["gamma-poisson-conjugate-family-48.html", "5.7 Gamma-Poisson conjugate family 4/8", " 5.7 Gamma-Poisson conjugate family 4/8 5.7.1 Joint probability mass function We have pmf for each day but if we want for \\(n\\) day we need to use joint probably mass function. (product of every pmf) \\[L(\\tau|\\overrightarrow{y}) = \\frac{\\tau^{\\sum y_i e^{-n\\tau}}}{\\prod_{i=n}^{n} y!} \\propto e^{-n\\tau} \\tau^{\\sum y_i}\\] We just need : \\(n\\) and \\(\\sum y_i\\) "],["gamma-poisson-conjugate-family-58.html", "5.8 Gamma-Poisson conjugate family 5/8", " 5.8 Gamma-Poisson conjugate family 5/8 5.8.1 Potential priors? \\(\\tau\\) is postif and continuous We have 3 probability models : Gamma : \\(f(\\tau) \\propto \\tau^{s-1} e^{-r\\tau}\\) Weibull : \\(f(\\tau) \\propto \\tau^{s-1} e^{(-r\\tau)^s}\\) F : \\(f(\\tau) \\propto \\tau^{s/2 - 1} (1 + \\tau) ^ -s\\) Quiz! Which one ? "],["gamma-poisson-conjugate-family-68.html", "5.9 Gamma-Poisson conjugate family 6/8", " 5.9 Gamma-Poisson conjugate family 6/8 5.9.1 Gamma prior : Gamma and Exponential models \\(\\tau\\) continuous random variable but can only take + value \\[\\tau \\sim Gamma(s, r)\\] Probability density functions: \\[f(\\tau) = \\frac{r^s}{\\Gamma (s)} \\tau^{s - 1} e^{-r\\tau} \\quad for \\quad \\tau &gt; 0 \\] \\[ E(\\tau) = \\frac{s}{r} ; Mode(\\tau) = \\frac{s - 1}{r} \\quad for \\quad s \\geq 1; Var(\\tau) = \\frac{s}{r^2} \\] When s = 1 -&gt; Exponenial model = Gamma(1,r) \\[\\tau \\sim Exp(r)\\] "],["gamma-poisson-conjugate-family-6n.html", "5.10 Gamma-Poisson conjugate family 6/n", " 5.10 Gamma-Poisson conjugate family 6/n 5.10.1 Quiz! Gamma when s &gt; r ? Gamme when s &lt; r ? More variability in Gamma(20, 20) or Gamma(20, 100) ? dashed = modes solid = means "],["gamma-poisson-conjugate-family-78.html", "5.11 Gamma-Poisson conjugate family 7/8", " 5.11 Gamma-Poisson conjugate family 7/8 5.11.1 Applications! \\[ E(\\tau) = \\frac{s}{r} \\approx 5\\] -&gt; we need \\(s = 5r\\) Trial and error: bayesrules::plot_gamma(shape = 10, rate = 2) Yeahhhh! we have a Prior! "],["gamma-poisson-conjugate-family-88.html", "5.12 Gamma-Poisson conjugate family 8/8", " 5.12 Gamma-Poisson conjugate family 8/8 5.12.1 Gamma-Poisson conjugacy Now need a posterior! \\[ \\tau|\\overrightarrow{y} \\sim Gamma(s + \\sum y_i, r +n) \\] We have: Gamma(10,2) and as data: \\(\\overrightarrow{y} = (6 + 2 + 2+ 1 )\\) , \\(n = 4\\) \\[\\sum_{i = 1}^{4} = 6 + 2 + 2 + 1 =11\\] \\[\\overline{y} = \\frac{\\sum_{i = 1}^{4}}{4} = 2.75\\] \\[L(\\tau|\\overrightarrow{y}) = \\frac{\\tau^ye^{-n\\tau}}{y!}\\] \\[L(\\tau|\\overrightarrow{y}) = \\frac{\\tau^{11}e^{-4\\tau}}{6!2!2!1!} \\propto \\tau^{11}e^{-4\\tau}\\] bayesrules::plot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10) We have prior, data, likelihood -&gt; posterior \\[Gamma(10, 2) \\longrightarrow Gamma(s + \\sum y_i, r +n)\\] \\[\\tau|\\overrightarrow{y} \\sim Gamma(21, 6) \\] bayesrules::plot_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4) "],["normal-normal-conjugate-family.html", "5.13 Normal-Normal conjugate family", " 5.13 Normal-Normal conjugate family TODO ‚Ä¶ Kitten Love GIFfrom Kitten GIFs "],["why-no-simulation-in-this-chapter.html", "5.14 Why no simulation in this chapter?", " 5.14 Why no simulation in this chapter? Hard to do! We moved from sample size 1 -&gt; \\(n\\) "],["critiques-of-conjugate-family.html", "5.15 Critiques of conjugate family", " 5.15 Critiques of conjugate family less flexible in selection the prior some does not allow flat prior "],["summary-1.html", "5.16 Summary", " 5.16 Summary conjugate priors are easy to compute/derive, interpretable Beta-Binomial: data Y is the number of successes in a set of \\(n\\) trials Gamma-Poisson: Y is a count with no upper limit Normal-Normal: Y is continuous "],["slide-1-2.html", "5.17 SLIDE 1", " 5.17 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-4.html", "5.18 Meeting Videos", " 5.18 Meeting Videos 5.18.1 Cohort 1 5.18.2 Cohort 2 Meeting chat log LOG "],["approximating-the-posterior.html", "Chapter 6 Approximating the Posterior", " Chapter 6 Approximating the Posterior Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-3.html", "6.1 SLIDE 1", " 6.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-5.html", "6.2 Meeting Videos", " 6.2 Meeting Videos 6.2.1 Cohort 1 6.2.2 Cohort 2 Meeting chat log LOG "],["mcmc-under-the-hood.html", "Chapter 7 MCMC under the Hood", " Chapter 7 MCMC under the Hood Learning objectives: Conceptual understanding of how Markov chain algorithms work Explore Metropolis-Hastings algorithm Implement it with the Normal-Normal "],["the-big-idea-12.html", "7.1 The big idea 1/2", " 7.1 The big idea 1/2 We are going to use a Normal-Normal model: \\[ Y|\\mu \\sim Norm(\\mu, 0.75^2)\\] \\[ \\mu \\sim Norm(0, 1^2) \\] Observed outcome 6.25: \\[ \\mu|(Y = 6.25) \\sim Norm(4, 0.6^2)\\] Main idea: chain need to spend more time around \\(\\mu\\) value. Remember \\(\\mu^{i+1}\\) is dependant of \\(\\mu^{i}\\). How are we going to visit every part of the posterior dustribution: step 1 : propose a random location \\(\\mu&#39;\\) (I prefer \\(\\mu_{proposal}\\)) for the nex stop step 2 : Decide whether to: go to the proposed location: \\(\\mu_{proposal} = \\mu^{i+1}\\) stay at the current location: \\(\\mu = \\mu^{i+1}\\) Monte Carlo algorithm: step 1 propose location: draw \\(\\mu\\) from posterior model with \\(pdf \\quad f(\\mu|y)\\) step 2 : Go there "],["the-big-idea-22.html", "7.2 The big idea 2/2", " 7.2 The big idea 2/2 But we are using MCMC to approximate this pdf (so we can‚Äôt sample it). For we are going to use two tricks: We do know that \\(f(u|y = 6.25) \\propto f(\\mu)L(u|y = 6.25)\\) For step 1 we can use an other model or distribution to generate proposals \\[ \\mu_{proposal}|\\mu \\sim Unif(\\mu - w, \\mu, + w) \\] with pdf: \\[ q(\\mu&#39;|\\mu) = \\frac{1}{2 w}\\] This give us a way for doing step1! "],["the-metropolis-hastings-algorithm.html", "7.3 The Metropolis-Hastings algorithm", " 7.3 The Metropolis-Hastings algorithm step 1: ok : \\(q(\\mu_{proposal}|\\mu)\\) step 2: we are going to calculate an acceptance probability: \\[\\alpha = \\{1, \\frac{f(\\mu_{proposal}) L(\\mu_{proposal}|y) q(\\mu|\\mu_{proposal})}{f(\\mu)L(\\mu|y) q(\\mu_{proposal|\\mu})} \\}\\] Because the Uniform proposal model is symmetric: \\[ q(\\mu_{proposal}|\\mu) = q(\\mu|\\mu_{proposal})\\] Then after multiplying by \\(f(y)\\) we have now: \\[ \\alpha = min\\{1, \\frac{f(\\mu_{proposal}|y)}{f(\\mu|y)} \\} \\] This ration is equivalent to the unnormalized posterior. Two scnearios: Scenario 1: \\(f(\\mu_{proposal}|y) \\geq f(\\mu|y)\\) -&gt; \\(\\alpha = 1\\) we are moving Scenario 2: $f(_{proposal}|y) &lt; f(|y) $ then we move according to the probability \\(\\alpha\\) one_mh_iteration &lt;- function(w, current){ # STEP 1: Propose the next chain location proposal &lt;- runif(1, min = current - w, max = current + w) # STEP 2: Decide whether or not to go there proposal_plaus &lt;- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75) current_plaus &lt;- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75) alpha &lt;- min(1, proposal_plaus / current_plaus) next_stop &lt;- sample(c(proposal, current), size = 1, prob = c(alpha, 1-alpha)) # Return the results return(data.frame(proposal, alpha, next_stop)) } set.seed(8) one_mh_iteration(w = 1, current = 3) ## proposal alpha next_stop ## 1 2.93259 0.8240205 2.93259 "],["implementing-the-metropolis-hastings.html", "7.4 Implementing the Metropolis-Hastings", " 7.4 Implementing the Metropolis-Hastings mh_tour &lt;- function(N, w){ # 1. Start the chain at location 3 current &lt;- 3 # 2. Initialize the simulation mu &lt;- rep(0, N) # 3. Simulate N Markov chain stops for(i in 1:N){ # Simulate one iteration sim &lt;- one_mh_iteration(w = w, current = current) # Record next location mu[i] &lt;- sim$next_stop # Reset the current location current &lt;- sim$next_stop } # 4. Return the chain locations return(data.frame(iteration = c(1:N), mu)) } library(ggplot2) set.seed(84735) mh_simulation_1 &lt;- mh_tour(N = 5000, w = 1) ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + geom_line() ggplot(mh_simulation_1, aes(x = mu)) + geom_histogram(aes(y = ..density..), color = &quot;white&quot;, bins = 20) + stat_function(fun = dnorm, args = list(4,0.6), color = &quot;blue&quot;) ## Tuning the Metropolis-Hastings algorithm 7.4.1 Quiz! w = 0.01 or w = 100, or w = 1 "],["a-beta-binomial-example.html", "7.5 A Beta-Binomial example", " 7.5 A Beta-Binomial example 1 success in 2 trials \\[ Y|\\pi = bin(2, \\pi) \\] \\[ \\pi = Beta(2,3) \\] We are still playing ‚Äúpretend‚Äù We are moving for step 1 to an Uniform to a Beta model because we want \\(\\pi\\) to be [0,1]. And we will draw every step from this Beta model. -&gt; change in step 1 \\[\\alpha = min \\{1, \\frac{f(\\pi_{proposal}|y)q(\\pi)}{f(\\pi|y) q(\\pi_{proposal})} \\} \\] one_iteration &lt;- function(a, b, current){ # STEP 1: Propose the next chain location proposal &lt;- rbeta(1, a, b) # STEP 2: Decide whether or not to go there proposal_plaus &lt;- dbeta(proposal, 2, 3) * dbinom(1, 2, proposal) proposal_q &lt;- dbeta(proposal, a, b) # &lt;- new current_plaus &lt;- dbeta(current, 2, 3) * dbinom(1, 2, current) current_q &lt;- dbeta(current, a, b) # &lt;- new alpha &lt;- min(1, proposal_plaus / current_plaus * current_q / proposal_q) next_stop &lt;- sample(c(proposal, current), size = 1, prob = c(alpha, 1-alpha)) return(data.frame(proposal, alpha, next_stop)) } betabin_tour &lt;- function(N, a, b){ # 1. Start the chain at location 0.5 current &lt;- 0.5 # 2. Initialize the simulation pi &lt;- rep(0, N) # 3. Simulate N Markov chain stops for(i in 1:N){ # Simulate one iteration sim &lt;- one_iteration(a = a, b = b, current = current) # Record next location pi[i] &lt;- sim$next_stop # Reset the current location current &lt;- sim$next_stop } # 4. Return the chain locations return(data.frame(iteration = c(1:N), pi)) } "],["why-the-algorithm-works.html", "7.6 Why the algorithm works", " 7.6 Why the algorithm works If the algorithm works : \\[\\frac{\\mu \\rightarrow \\mu_{proposa}}{\\mu_{proposal} \\rightarrow \\mu} = \\frac{ f(\\mu_{proposal}|y)}{f(\\mu|y)}\\] "],["chapter-summary.html", "7.7 Chapter summary", " 7.7 Chapter summary Step 1 Propose a new chain location by drawing from a proposal pdf which is perhaps dependent upon the current location Determine whether accept the proposal: depends on how favorable its posterior plausibility is relative to the posterior plausibility of the current location "],["meeting-videos-6.html", "7.8 Meeting Videos", " 7.8 Meeting Videos 7.8.1 Cohort 1 Meeting chat log 00:00:12 Federica Gazzelloni: Hello! 00:58:29 olivier leroy: while (count_accept &lt; M) { count_draw &lt;- count_draw + 1 un_runif &lt;- runif(1) k &lt;- rbinom(prob = un_runif, size = 10, n = 1) if(k == kobs) { count_accept &lt;- count_accept + 1 post[count_accept] &lt;- un_runif } } 01:04:14 olivier leroy: https://www.youtube.com/watch?v=Qqz5AJjyugM&amp;list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&amp;index=8&amp;pp=sAQB 7.8.2 Cohort 2 Meeting chat log LOG "],["posterior-inference-prediction.html", "Chapter 8 Posterior Inference &amp; Prediction", " Chapter 8 Posterior Inference &amp; Prediction Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-4.html", "8.1 SLIDE 1", " 8.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-7.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log LOG "],["simple-normal-regression.html", "Chapter 9 Simple Normal Regression", " Chapter 9 Simple Normal Regression Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-5.html", "9.1 SLIDE 1", " 9.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-8.html", "9.2 Meeting Videos", " 9.2 Meeting Videos 9.2.1 Cohort 1 Meeting chat log LOG "],["evaluating-regression-models.html", "Chapter 10 Evaluating Regression Models", " Chapter 10 Evaluating Regression Models Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-6.html", "10.1 SLIDE 1", " 10.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-9.html", "10.2 Meeting Videos", " 10.2 Meeting Videos 10.2.1 Cohort 1 Meeting chat log LOG "],["extending-the-normal-regression-model.html", "Chapter 11 Extending the Normal Regression Model", " Chapter 11 Extending the Normal Regression Model Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-7.html", "11.1 SLIDE 1", " 11.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-10.html", "11.2 Meeting Videos", " 11.2 Meeting Videos 11.2.1 Cohort 1 Meeting chat log LOG "],["poisson-negative-binomial-regression.html", "Chapter 12 Poisson &amp; Negative Binomial Regression", " Chapter 12 Poisson &amp; Negative Binomial Regression Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-8.html", "12.1 SLIDE 1", " 12.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-11.html", "12.2 Meeting Videos", " 12.2 Meeting Videos 12.2.1 Cohort 1 Meeting chat log LOG "],["logistic-regression.html", "Chapter 13 Logistic Regression", " Chapter 13 Logistic Regression Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-9.html", "13.1 SLIDE 1", " 13.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-12.html", "13.2 Meeting Videos", " 13.2 Meeting Videos 13.2.1 Cohort 1 Meeting chat log LOG "],["naive-bayes-classification.html", "Chapter 14 Naive Bayes Classification", " Chapter 14 Naive Bayes Classification Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-10.html", "14.1 SLIDE 1", " 14.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-13.html", "14.2 Meeting Videos", " 14.2 Meeting Videos 14.2.1 Cohort 1 Meeting chat log LOG "],["hierarchical-models-are-exciting.html", "Chapter 15 Hierarchical Models are Exciting", " Chapter 15 Hierarchical Models are Exciting Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-11.html", "15.1 SLIDE 1", " 15.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-14.html", "15.2 Meeting Videos", " 15.2 Meeting Videos 15.2.1 Cohort 1 Meeting chat log LOG "],["normal-hierarchical-models-without-predictors.html", "Chapter 16 (Normal) Hierarchical Models without Predictors", " Chapter 16 (Normal) Hierarchical Models without Predictors Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-12.html", "16.1 SLIDE 1", " 16.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-15.html", "16.2 Meeting Videos", " 16.2 Meeting Videos 16.2.1 Cohort 1 Meeting chat log LOG "],["normal-hierarchical-models-with-predictors.html", "Chapter 17 (Normal) Hierarchical Models with Predictors", " Chapter 17 (Normal) Hierarchical Models with Predictors Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-13.html", "17.1 SLIDE 1", " 17.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-16.html", "17.2 Meeting Videos", " 17.2 Meeting Videos 17.2.1 Cohort 1 Meeting chat log LOG "],["non-normal-hierarchical-regression-classification.html", "Chapter 18 Non-Normal Hierarchical Regression &amp; Classification", " Chapter 18 Non-Normal Hierarchical Regression &amp; Classification Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-14.html", "18.1 SLIDE 1", " 18.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-17.html", "18.2 Meeting Videos", " 18.2 Meeting Videos 18.2.1 Cohort 1 Meeting chat log LOG "],["adding-more-layers.html", "Chapter 19 Adding More Layers", " Chapter 19 Adding More Layers Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-15.html", "19.1 SLIDE 1", " 19.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-18.html", "19.2 Meeting Videos", " 19.2 Meeting Videos 19.2.1 Cohort 1 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
