[["index.html", "Bayes Rules! Book Club Welcome", " Bayes Rules! Book Club The R4DS Online Learning Community 2022-10-06 Welcome Welcome to the bookclub! This is a companion for the book Bayes Rules! by Alicia A. Johnson, Miles Q. Ott, and Mine Dogucu (Chapman and Hall/CRC, copyright 2022, 9780367255398). This companion is available at r4ds.io/bayes_rules. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We‚Äôll try to cover 1 chapter/week, but‚Ä¶ ‚Ä¶It‚Äôs ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. Following the flow! Source: https://www.youtube.com/watch?v=zYYBtxHWE0A From: Richard McElreath, Statistical Rethinking videos "],["preface.html", "Preface ", " Preface "],["bayesian-statistics.html", "0.1 Bayesian statistics?", " 0.1 Bayesian statistics? Frequentist and Bayesian methods share: learning from data But Bayesian allows: new data + prior results easier to interpret shines when frequentist fails computational tools more accesible now "],["tips-and-tricks-from-the-authors.html", "0.2 Tips and tricks from the authors", " 0.2 Tips and tricks from the authors Learn by doing Embrace a growth mindset (we will do mistakes!) Interpret Bayes in a context (ethics and maybe more) Practice, practice, practice "],["set-up.html", "0.3 Set up", " 0.3 Set up Install rstan : https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started install.packages(c(&quot;bayesrules&quot;, &quot;tidyverse&quot;, &quot;janitor&quot;, &quot;rstanarm&quot;, &quot;bayesplot&quot;, &quot;tidybayes&quot;, &quot;broom.mixed&quot;, &quot;modelr&quot;, &quot;e1071&quot;, &quot;forcats&quot;), dependencies = TRUE) On linux (ubuntu 22) I had to update some dependencies. "],["the-authors.html", "0.4 The authors:", " 0.4 The authors: Alicia A. Johnson : Website https://ajohns24.github.io/portfolio/ Miles Q. Ott: https://twitter.com/Miles_Ott Mine Dogucu: https://twitter.com/MineDogucu "],["the-big-bayesian-picture.html", "Chapter 1 The Big (Bayesian) Picture", " Chapter 1 The Big (Bayesian) Picture Learning objectives: Learn to think like a Bayesian. Explore the foundations of a Bayesian data analysis and how they contrast with the frequentist alternative Learn a little bit about the history of the Bayesian philosophy "],["thinking-like-a-bayesian-14.html", "1.1 Thinking like a Bayesian 1/4", " 1.1 Thinking like a Bayesian 1/4 DiagrammeR::grViz(&quot; digraph thinking_bayesian{ # node statement node [shape = oval] a [label = &#39;Prior&#39;]; b [label = &#39;Data&#39;]; c [label = &#39;Posterior&#39;]; d [label = &#39;New data&#39;]; e [label = &#39;Posterior&#39;]; f [label = &#39;New data&#39;] g [style = invisible ] # edge statement a -&gt; c b -&gt; c c -&gt; e d -&gt; e f-&gt; g [style = dashed] e-&gt; g [style = dashed] }&quot;) Figure 1.1: A Bayesian knowledge-building diagram Both Bayesian and frequentist share a common goal: learn from data about the world around Both use data to fit nodels, make predictions and evaluate hypothesis "],["quiz-time.html", "1.2 Quiz time!", " 1.2 Quiz time! 4-5: frequentist 6-8: a bit of both 9-12: Bayesian TODO link to script and data in repo "],["thinking-like-a-bayesian-24.html", "1.3 Thinking like a Bayesian 2/4", " 1.3 Thinking like a Bayesian 2/4 1.3.1 Interpreting probability: Bayesian philosophy: relative plausibility of an event Frequentist philosophy: long-run relative frequency of a repeatable event "],["thinking-like-a-bayesian-34.html", "1.4 Thinking like a Bayesian 3/4", " 1.4 Thinking like a Bayesian 3/4 1.4.1 Bayesian balancing act Two claims: Zuofo claims he can predict the outcome of coin flip Kavya claims she can distinguish between natural and artificial sweeteners If both succeed with a 10/10 sucess rate what can we conclude from this? The frequentist approach will discard prior knowledge (it is harder to predict coin flip that having a sensitive palate to sweeteners) and the Bayesian want to use this prior knowledge. -&gt; How can we balance Prior and Data? "],["thinking-like-a-bayesian-44.html", "1.5 Thinking like a Bayesian 4/4", " 1.5 Thinking like a Bayesian 4/4 1.5.1 Asking question What‚Äôs the chance that I actually have the disease (a)? Versus I do not have the disease, What‚Äôs the chance that I would have gotten this positive test results (b)? # building data disease &lt;- c(rep(&quot;disease&quot;, 4), rep(&quot;no disease&quot;, 96)) a &lt;- &quot;test positive&quot; ; b &lt;- &quot;test negative&quot; test &lt;- c(rep(a, 3), b, rep(a, 9), rep(b, 87)) disease_status &lt;- data.frame(disease, test) # contingency table contingency_disease &lt;- table(disease_status) contingency_disease &lt;- addmargins(contingency_disease) knitr::kable(contingency_disease ) test negative test positive Sum disease 1 3 4 no disease 87 9 96 Sum 88 12 100 (a): 3 / 12 (b): 9 / 96 Analogy between (b) and p-value: it is more natural to study the uncertainty of a yet-unproven hypothesis than the uncertainty of data we have already observed.(authors‚Äôopinion) "],["quick-history-lesson.html", "1.6 Quick history lesson", " 1.6 Quick history lesson From stigmatized to being used in modeling COVID-19 rates. Why? advances in computing departure from tradition (what people learn is what people use) reevaluation of subjectivity : frequentist is also subjective and subjectivity is not any more a dirty word. "],["look-ahead.html", "1.7 Look ahead", " 1.7 Look ahead 1.7.1 4 units Bayesian foundations: 5 chapters Focus: models &amp; distributions (conjugate family) Posterior simulations &amp; analysis: 3 chapters Focus: when conjugate is not an option: MCMC then posterior analysis Bayesian regression &amp; classification Focus: extending unit 1 reponse variable (Y) with predictor variables (X) Hierarchical Bayesian models Focus: expanding unit 3 to accomodate and harness grouped data. "],["summary.html", "1.8 Summary", " 1.8 Summary Posterior knowledge &lt;- balancing information from data and prior knowledge More ‚Äúwaves‚Äùof data -&gt; refine knowledge (less effect of prior) With more and more data, two analysts will converge on the same posterior knowledge "],["resources-mentioned.html", "1.9 Resources mentioned", " 1.9 Resources mentioned This is a list of resources mentioned in the first meeting! 1.9.1 Other Bayesian books: Richard McElreath * book: https://xcelab.net/rm/statistical-rethinking/ * vid√©o: https://github.com/rmcelreath/stat_rethinking_2022 The ‚Äúpuppy‚Äù book by John K. Kruschke : https://sites.google.com/site/doingbayesiandataanalysis/ Introduction to Bayesian Thinking, Clyde, Centinkaya-Rundel et al similar level to our book Bayesian Data Analysis, Andrew Gelman more precise (and mathematical) Intro to Bayes Theorem, Wrath of Math, video Clear explanation of the meaning of given in statements like probability of A given B. $ P(A | B) $ 1.9.2 Drawing DAG (Directed Acyclic Graph) Pen and paper DiagrammeR: for drawing diagram, uses Graphviz or mermaid Dagitty: for causal diagrams 1.9.3 Podcast Learning Bayesian Statistics ep. 42 With Mine Dogucu "],["meeting-videos.html", "1.10 Meeting Videos", " 1.10 Meeting Videos 1.10.1 Cohort 1 Meeting chat log 00:06:01 Olivier: hello ! 00:06:47 Olivier: various links : https://r4ds.github.io/bookclub-bayes_rules/ 00:06:55 Olivier: https://docs.google.com/spreadsheets/d/18IDSOU2bfkD55kOB18qCB7Idbpiyp4_9qeWjkvE-Syc/edit#gid=0 00:09:44 Olivier: Let≈õ wait that everyone configure zoom :P 00:10:18 Gabby Palomo: I didn&#39;t see if anyone else signed up for this cohort. I imagine more people did, right? 00:10:46 Olivier: I do not know the number of participant 00:10:52 Olivier: I will have to ask 00:11:08 Will: Well hopefully. I&#39;m assuming for fellow Brits there are lots people celebrating the jubilee. 00:11:59 Gabby Palomo: ah that&#39;s true!! 00:12:10 erik.aalto@tocaboca.com: I‚Äôm not hearing anything, but I could hear recording in progress when I joined.. 00:12:24 Olivier: do you hear me ? 00:12:27 Gabby Palomo: There are 85 people in the slack channel so will see. 00:12:42 erik.aalto@tocaboca.com: Nope, cant hear anything:/ 00:13:15 Will: Yes I think I can hear Erik 00:13:45 erik.aalto@tocaboca.com: Darn, can‚Äôt hear anything‚Ä¶this is weird 00:13:57 Olivier: we hear you at least 00:14:03 erik.aalto@tocaboca.com: I hear the zoom notifs‚Ä¶like ‚Äùrecording in progress‚Äù 00:15:52 Olivier: it is fine 00:16:13 Olivier: is it better 00:17:24 Olivier: is it good now ? 00:17:41 Olivier: i restaart it 00:19:02 Olivier: working or not ? 00:19:20 Ronald Legere: Not yet‚Ä¶ there is a audio test thing in the audio settings menu 00:19:48 Olivier: good name 00:56:58 Ronald Legere: Can you link those podcasts to slack? Or here ;) 01:06:31 Olivier Leroy: DiagrammeR 1.10.2 Cohort 2 "],["bayes-rule.html", "Chapter 2 Bayes‚Äô Rule", " Chapter 2 Bayes‚Äô Rule Learning objectives: Explore foundational probability tools conditional probability: Probability of A given B \\(P(A|B)\\) joint probability: Probability of A and B occuring \\(P(A \\cap B)\\) marginal probability: Probability of an event \\(P(A)\\) Law of Total Probability: If a probability of an event is unknown it can be calculated using the know probability of other related event Conduct first formal Bayesian analysis Practice your Bayesian grammar Prior Likelihood Normalizing constant Simulate Bayesian models sample() sample_n() rbinon() "],["building-a-bayesian-model-for-events.html", "2.1 Building a Bayesian model for events", " 2.1 Building a Bayesian model for events First Data set ?? fake_news Figure 2.1: Bayesian knowledge-building diagram for wether or not the article is fake Two variables: - fake vs real - ! or not 2.1.1 Workflow: Prior probability model A model for interpreting the data Posterior probability model \\[ P(FakeNew = 0.4 ) \\quad and \\quad P(Real = 0.6) \\] \\[ P(B = 0.4 ) \\quad and \\quad P(B^c = 0.6) \\] Here \\(P(FakeNew)\\) : prior probability of an article to be a fake news Valid probability model : 1. accounts all event 2. assign probabilities for each event 3. sum to one \\(P(ExClam)\\) : probability that an article contains an exclamation mark in his title We know that if an article is fake news : 26.67% that the title contains ! and if it is not fake this is just 2.22%. \\[ P(Exclam|FakeNew) = 0.2667 \\quad and \\quad P(Exclam|Real = 0.0222) \\] This is a conditional probability. Conditional probability help know if B give us insight in A. If it does not provide any information it means that A and B event are independent (\\(P(A|B) = P(A)\\)). "],["normalizing-constant.html", "2.2 Normalizing constant", " 2.2 Normalizing constant \\(P(Exclam|FakeNew) = 0.2667 \\quad and \\quad P(Exclam|Real) = 0.0222)\\) are our likelihood, when we know A (!) we know that getting B (Fake news) is more likely. It is different that our prior probability. Then we need can calculate the joint probability (probability of observing A nd B for example), of each options (here it is half of them): \\[ P(Exclam \\cap FakeNew) = P(Exclam|FakeNew) P (Fakenew) = 0.2667 *0.4 = 0.1067 \\] \\[ P(PasExclam \\cap FakeNew ) = (1 - P(Exclam|FakeNew)) * P(FakeNew)) = (1 - 0.2667) * 0.4 = 0.2993 \\] Here \\(P(B)\\) is the marginal probability of B B &lt;- c(0.1067, 0.2933, 0.4) Bc &lt;- c(0.0133, 0.5867, 0.6) Total &lt;- c(0.12, 0.88, 1) joint_p &lt;- data.frame(B, Bc, Total, row.names = c(&quot;A&quot;, &quot;Ac&quot;, &quot;Total&quot;)) knitr::kable(joint_p) B Bc Total A 0.1067 0.0133 0.12 Ac 0.2933 0.5867 0.88 Total 0.4000 0.6000 1.00 \\(P(Exclam)\\) is our normalizing constant. Ok but we want \\(P(FakeNew|exlam)\\) ie \\(P(B|A)\\) \\[ P(FakeNew|exclam) = \\frac{P(exclam \\cap FakeNew)}{P(Exclam)} = \\frac{P(FakeNew)L(FakeNew|Exclam)}{P(Exclam)} \\] \\[ posterior = \\frac{prior . likelihood}{normalizing \\quad constant} = \\frac{0.4 * 0.2667}{0.12} = 0.889 \\] "],["posterior-simulation.html", "2.3 Posterior simulation", " 2.3 Posterior simulation This was a model. Now we will use a simulation! library(dplyr) library(ggplot2) set.seed(84735) # Define possible articles article &lt;- data.frame(type = c(&quot;real&quot;, &quot;fake&quot;)) # Define the prior model prior &lt;- c(0.6, 0.4) article_sim &lt;- dplyr::sample_n(article, size = 10000, weight = prior, replace = TRUE) # dats model article_sim &lt;- article_sim %&gt;% mutate(data_model = case_when(type == &quot;fake&quot; ~ 0.2667, type == &quot;real&quot; ~ 0.0222)) # Simulate exclamation point usage data &lt;- c(&quot;NoExclam&quot;, &quot;Exclam&quot;) set.seed(3) # Rbase simplier ? article_sim &lt;- article_sim %&gt;% group_by(1:n()) %&gt;% mutate(usage = sample(data, size = 1, prob = c(1 - data_model, data_model))) ggplot(article_sim, aes(x = type)) + geom_bar() + facet_wrap(~ usage) "],["example-pop-vs-soda-vs-coke.html", "2.4 Example Pop vs Soda vs Coke", " 2.4 Example Pop vs Soda vs Coke Expend TRUE/FALSE example with one with categories. "],["building-a-bayesian-model-for-random-variables-1n.html", "2.5 Building a Bayesian model for random variables (1/n)", " 2.5 Building a Bayesian model for random variables (1/n) 2.5.1 First step prior \\(\\pi\\) : skill of Kasparov relative to Deep Blue (random variable) Prior model of \\(\\pi\\): pi &lt;- c(0.2, 0.5, 0.8, &quot;total&quot;) # pmf : probability mass functions pmf &lt;- c(0.1, 0.25, 0.65, 1) prior_pi &lt;- data.frame(pi, pmf) knitr::kable(t(prior_pi)) pi 0.2 0.5 0.8 total pmf 0.10 0.25 0.65 1.00 "],["building-a-bayesian-model-for-random-variables-1n-1.html", "2.6 Building a Bayesian model for random variables (1/n)", " 2.6 Building a Bayesian model for random variables (1/n) 2.6.1 Binomial data model Y is the number of games (on 6 games) that Kasparov wins. Y our random variable: {0, 1, ‚Ä¶., 6} , depends on \\(\\pi\\) \\[f(y|\\pi) = P (Y = y|\\pi ) \\] \\(y\\) : any possible outcone If we assume games are independents (no effect on each other) and \\(\\pi\\) is fixed we can use the Binomial model. Y is the number of successes in a fixed number of trials (\\(n\\)) \\[ Y|\\pi \\sim Bin(n, \\pi) \\] \\[ f(y|\\pi) = \\begin{pmatrix} 6 \\\\y \\end{pmatrix} \\pi^y(1 - \\pi)^{6 - y} \\quad for \\quad y \\in \\begin{Bmatrix} 0, 1, 2, 3, 4, 5, 6 \\end{Bmatrix} \\] We can use the prior for \\(\\pi\\) and all \\(y\\) to calculate each probabilities. "],["building-a-bayesian-model-for-random-variables-1n-2.html", "2.7 Building a Bayesian model for random variables (1/n)", " 2.7 Building a Bayesian model for random variables (1/n) 2.7.1 Binomial likelihood function Kasparov only won one of six games. This is our data with \\(L(\\pi|y = 1)\\). We can calculate it for each \\(\\pi\\) value. What is more likely is that \\(pi\\) was 0.2. 2.7.2 Probability mass functions vs likelihood functions When \\(\\pi\\) is known the conditional pmf allows us to compare the probabilities of different possible value of data Y (y1, y2 ..) occuring with \\(pi\\) when Y = y is known the likelihood function allows us to to compare the relative values of \\(\\pi\\) (\\(\\pi_1, \\pi_2, etc ..\\)) 2.7.3 Normalizing constant Total probability that Kasparov would win Y = 1 game across all possible win probability of \\(\\pi\\) We apply the Law of Total Probability (the sum of all likelihood for each value of \\(\\pi\\) by the prior probailities of these \\(\\pi\\) values) \\[ f(y = 1) = L(\\pi = 0.2 | y = 1) f(\\pi = 0.2 ) + L(\\pi = 0.5 | y = 1) f(\\pi = 0.5 ) + L(\\pi = 0.8 | y = 1) f(\\pi = 0.8 ) +\\] \\[ f( y = 1) \\simeq 0.3932 * 0.1 + 0.0938 * 0.25 + 0.0015 * 0.65 \\simeq 0.0637 \\] 2.7.4 Posterior probability model We have the prior, the likelihod and the normalizing constant -&gt; Bayes Rules! \\[ posterior = \\frac{prior . likelihood}{normalizing \\quad constant} \\quad for \\in \\begin{Bmatrix} 0.2, 0 5, 0.8 \\end{Bmatrix} \\] 2.7.5 Posterior shortcut The normalizing constant, is a constant it appears in all posterior calculations. We can work with unnormalizied proabilities or we can divide each unnormalizied probability by the sum of each of them. \\[ posterior \\propto prior * likelihood \\] ## Posterior simulation # Define possible win probabilities chess &lt;- data.frame(pi = c(0.2, 0.5, 0.8)) # Define the prior model prior &lt;- c(0.10, 0.25, 0.65) # Simulate 10000 values of pi from the prior set.seed(84735) chess_sim &lt;- sample_n(chess, size = 10000, weight = prior, replace = TRUE) chess_sim &lt;- chess_sim %&gt;% mutate(y = rbinom(10000, size = 6, prob = pi)) # Focus on simulations with y = 1 win_one &lt;- chess_sim %&gt;% filter(y == 1) # Plot the posterior approximation ggplot(win_one, aes(x = pi)) + geom_bar() "],["links-shared-in-the-second-meeting.html", "2.8 Links shared in the second meeting", " 2.8 Links shared in the second meeting Bayesian probability for babies! (with cookies) : https://raw.githubusercontent.com/epimath/epid-814-materials/master/Lectures/BayesianEstimation.pdf Author: Marisa Eisenberg "],["meeting-videos-1.html", "2.9 Meeting Videos", " 2.9 Meeting Videos 2.9.1 Cohort 1 Meeting chat log 00:11:44 erik.aalto@tocaboca.com: Hello, sorry for joining late, happy to see you again üòÑ 00:36:06 Brendan Lam: https://www.youtube.com/watch?v=HZGCoVF3YvM 00:37:26 Erik: we‚Äôll utilize the following likelihood function notation &lt;poorly formatted paste&gt; 00:37:41 Erik: sorry for how the formatting above went‚Ä¶ 00:42:42 Lisa: to add to brendan&#39;s resource, here is a slide deck using cookies as an example: https://github.com/epimath/epid-814-materials/blob/master/Lectures/BayesianEstimation.pdf 01:02:00 Federica Gazzelloni: thanks 01:02:34 Federica Gazzelloni: I‚Äôd like to go back to the L(..|..) 01:02:35 Gabby Palomo: It was clear Olivier. Don‚Äôt worry!! 01:03:07 Federica Gazzelloni: just to understand how it works and what is the difference 01:03:43 Erik: Need to jump out! Thanks for today. 01:04:15 Lisa: I need to get back to work too. thank you for today! 01:04:58 Federica Gazzelloni: thanks 01:05:03 Federica Gazzelloni: I‚Äôll check that 2.9.2 Cohort 2 "],["the-beta-binomial-bayesian-model.html", "Chapter 3 The Beta-Binomial Bayesian Model", " Chapter 3 The Beta-Binomial Bayesian Model Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1.html", "3.1 SLIDE 1", " 3.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-2.html", "3.2 Meeting Videos", " 3.2 Meeting Videos 3.2.1 Cohort 1 Meeting chat log 00:10:55 Kasia Ozga: host disabled whitboard so we will not try it üòõ 00:30:25 Will Parbury: 3 Blue 1 Brown: Binomial distributions | Probabilities of probabilities, part 1 https://www.youtube.com/watch?v=8idr1WZ1A7Q Serrano.Academy: The Beta Distribution in 12 minutes! https://www.youtube.com/watch?v=juF3r12nM5A PsychEd: Milgram‚Äôs Obedience Experiment https://www.youtube.com/watch?v=cBDkJ-Nc3Ig 00:30:38 Federica Gazzelloni: thanks 00:35:29 Federica Gazzelloni: you can watch the fist two videos of advancedR cohort6 book club for how to make the notes 00:36:32 Federica Gazzelloni: https://www.youtube.com/playlist?list=PL3x6DOfs2NGjnCxGKeDNJUfPpRFI2hJjv 00:36:58 Lisa: thanks! 00:38:21 Federica Gazzelloni: to make the notes have a look at the first and the third videos 00:38:42 Federica Gazzelloni: just the begin 00:40:39 Kasia Ozga: v 00:40:52 Kasia Ozga: https://ben18785.shinyapps.io/distribution-zoo/ 00:44:06 Kasia Ozga: mean is alpha / (alpha +alpha) 00:44:25 Kasia Ozga: 0,4 = 1/alpha 3.2.2 Cohort 2 "],["balance-and-sequentiality-in-bayesian-analyses.html", "Chapter 4 Balance and Sequentiality in Bayesian Analyses", " Chapter 4 Balance and Sequentiality in Bayesian Analyses Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-1.html", "4.1 SLIDE 1", " 4.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log 00:14:08 Brendan Lam: eventually! 4.2.2 Cohort 2 "],["conjugate-families.html", "Chapter 5 Conjugate Families", " Chapter 5 Conjugate Families Learning objectives: Practice building Bayesian models Familiarize yourself with conjugacy "],["greek-letters.html", "5.1 Greek letters", " 5.1 Greek letters \\(\\lambda\\) = lambda \\(\\mu\\) = mu \\(\\sigma\\) = sigma \\(\\tau\\) = tau \\(\\theta\\) = theta This our last chapter on Bayesian foundations! "],["revisiting-choice-of-prior.html", "5.2 Revisiting choice of prior", " 5.2 Revisiting choice of prior Flexibility Computational ease: posterior easy to build Interpretability 5.2.1 Reminder the Beta-Binomial Model: Prior: \\(Beta (\\alpha, \\beta)\\) Data model: \\(Y = y \\quad for \\quad Bin(n, \\pi)\\) Posterior: \\(Beta(\\alpha + y, \\beta = n - y)\\) "],["joy.html", "5.3 Joy!", " 5.3 Joy! (yes I know it was the other way!) "],["gamma-poisson-conjugate-family-18.html", "5.4 Gamma-Poisson conjugate family 1/8", " 5.4 Gamma-Poisson conjugate family 1/8 We are going to do a model to estimate the number of fraud risk phone call: 5.4.1 Prior: rate \\(\\tau \\approx\\) 5 number of phone call / day can range from 2-7 "],["gamma-poisson-conjugate-family-28.html", "5.5 Gamma-Poisson conjugate family 2/8", " 5.5 Gamma-Poisson conjugate family 2/8 5.5.1 Poisson data model: \\(Y =\\) number of independant event that occur in a fixed ammount of time \\[Y|y \\sim Pois(\\tau) \\] Probability mass function: \\[ f(y|Y) = \\frac{\\tau^ye^-\\tau}{y!} \\quad for y \\in \\{0, 1, 2, ...\\} \\] (sum to 1) \\[E(Y|\\tau) = Var(Y|\\tau) = \\tau \\] "],["gamma-poisson-conjugate-family-38.html", "5.6 Gamma-Poisson conjugate family 3/8", " 5.6 Gamma-Poisson conjugate family 3/8 5.6.1 Poisson pmfs with different \\(\\tau\\) "],["gamma-poisson-conjugate-family-48.html", "5.7 Gamma-Poisson conjugate family 4/8", " 5.7 Gamma-Poisson conjugate family 4/8 5.7.1 Joint probability mass function We have pmf for each day but if we want for \\(n\\) day we need to use joint probably mass function. (product of every pmf) \\[L(\\tau|\\overrightarrow{y}) = \\frac{\\tau^{\\sum y_i e^{-n\\tau}}}{\\prod_{i=n}^{n} y!} \\propto e^{-n\\tau} \\tau^{\\sum y_i}\\] We just need : \\(n\\) and \\(\\sum y_i\\) "],["gamma-poisson-conjugate-family-58.html", "5.8 Gamma-Poisson conjugate family 5/8", " 5.8 Gamma-Poisson conjugate family 5/8 5.8.1 Potential priors? \\(\\tau\\) is postif and continuous We have 3 probability models : Gamma : \\(f(\\tau) \\propto \\tau^{s-1} e^{-r\\tau}\\) Weibull : \\(f(\\tau) \\propto \\tau^{s-1} e^{(-r\\tau)^s}\\) F : \\(f(\\tau) \\propto \\tau^{s/2 - 1} (1 + \\tau) ^ -s\\) Quiz! Which one ? "],["gamma-poisson-conjugate-family-68.html", "5.9 Gamma-Poisson conjugate family 6/8", " 5.9 Gamma-Poisson conjugate family 6/8 5.9.1 Gamma prior : Gamma and Exponential models \\(\\tau\\) continuous random variable but can only take + value \\[\\tau \\sim Gamma(s, r)\\] Probability density functions: \\[f(\\tau) = \\frac{r^s}{\\Gamma (s)} \\tau^{s - 1} e^{-r\\tau} \\quad for \\quad \\tau &gt; 0 \\] \\[ E(\\tau) = \\frac{s}{r} ; Mode(\\tau) = \\frac{s - 1}{r} \\quad for \\quad s \\geq 1; Var(\\tau) = \\frac{s}{r^2} \\] When s = 1 -&gt; Exponenial model = Gamma(1,r) \\[\\tau \\sim Exp(r)\\] "],["gamma-poisson-conjugate-family-6n.html", "5.10 Gamma-Poisson conjugate family 6/n", " 5.10 Gamma-Poisson conjugate family 6/n 5.10.1 Quiz! Gamma when s &gt; r ? Gamme when s &lt; r ? More variability in Gamma(20, 20) or Gamma(20, 100) ? dashed = modes solid = means "],["gamma-poisson-conjugate-family-78.html", "5.11 Gamma-Poisson conjugate family 7/8", " 5.11 Gamma-Poisson conjugate family 7/8 5.11.1 Applications! \\[ E(\\tau) = \\frac{s}{r} \\approx 5\\] -&gt; we need \\(s = 5r\\) Trial and error: bayesrules::plot_gamma(shape = 10, rate = 2) Yeahhhh! we have a Prior! "],["gamma-poisson-conjugate-family-88.html", "5.12 Gamma-Poisson conjugate family 8/8", " 5.12 Gamma-Poisson conjugate family 8/8 5.12.1 Gamma-Poisson conjugacy Now need a posterior! \\[ \\tau|\\overrightarrow{y} \\sim Gamma(s + \\sum y_i, r +n) \\] We have: Gamma(10,2) and as data: \\(\\overrightarrow{y} = (6 + 2 + 2+ 1 )\\) , \\(n = 4\\) \\[\\sum_{i = 1}^{4} = 6 + 2 + 2 + 1 =11\\] \\[\\overline{y} = \\frac{\\sum_{i = 1}^{4}}{4} = 2.75\\] \\[L(\\tau|\\overrightarrow{y}) = \\frac{\\tau^ye^{-n\\tau}}{y!}\\] \\[L(\\tau|\\overrightarrow{y}) = \\frac{\\tau^{11}e^{-4\\tau}}{6!2!2!1!} \\propto \\tau^{11}e^{-4\\tau}\\] bayesrules::plot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10) We have prior, data, likelihood -&gt; posterior \\[Gamma(10, 2) \\longrightarrow Gamma(s + \\sum y_i, r +n)\\] \\[\\tau|\\overrightarrow{y} \\sim Gamma(21, 6) \\] bayesrules::plot_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4) "],["normal-normal-conjugate-family.html", "5.13 Normal-Normal conjugate family", " 5.13 Normal-Normal conjugate family TODO ‚Ä¶ Kitten Love GIFfrom Kitten GIFs "],["why-no-simulation-in-this-chapter.html", "5.14 Why no simulation in this chapter?", " 5.14 Why no simulation in this chapter? Hard to do! We moved from sample size 1 -&gt; \\(n\\) "],["critiques-of-conjugate-family.html", "5.15 Critiques of conjugate family", " 5.15 Critiques of conjugate family less flexible in selection the prior some does not allow flat prior "],["summary-1.html", "5.16 Summary", " 5.16 Summary conjugate priors are easy to compute/derive, interpretable Beta-Binomial: data Y is the number of successes in a set of \\(n\\) trials Gamma-Poisson: Y is a count with no upper limit Normal-Normal: Y is continuous "],["slide-1-2.html", "5.17 SLIDE 1", " 5.17 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-4.html", "5.18 Meeting Videos", " 5.18 Meeting Videos 5.18.1 Cohort 1 5.18.2 Cohort 2 ## ‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ ## ‚úî tibble 3.1.8 ‚úî purrr 0.3.5 ## ‚úî tidyr 1.2.1 ‚úî stringr 1.4.1 ## ‚úî readr 2.1.3 ‚úî forcats 0.5.2 ## ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ ## ‚úñ dplyr::filter() masks stats::filter() ## ‚úñ dplyr::lag() masks stats::lag() ## ## Attaching package: &#39;janitor&#39; ## ## ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test ## ## ## Loading required package: StanHeaders ## ## rstan (Version 2.21.7, GitRev: 2e1f913d3ca3) ## ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## ## ## Attaching package: &#39;rstan&#39; ## ## ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## ## ## This is bayesplot version 1.9.0 ## ## - Online documentation and vignettes at mc-stan.org/bayesplot ## ## - bayesplot theme set to bayesplot::theme_default() ## ## * Does _not_ affect other ggplot2 plots ## ## * See ?bayesplot_theme_set for details on theme setting "],["approximating-the-posterior.html", "Chapter 6 Approximating the Posterior", " Chapter 6 Approximating the Posterior Learning objectives: Implement and examine the limitations of using grid approximation to simulate a posterior model. Explore the MCMC posterior simulation using R Learn several Markov chain diagnostics for examining the quality of an MCMC posterior simulation. N.B. We will learn more about how MCMC works in the next chapter! "],["motivation-for-approximations.html", "6.1 Motivation for approximations", " 6.1 Motivation for approximations Remember we are trying to compute the posterior distribution: \\[ f\\left(\\theta | y \\right) = \\frac{f(\\theta)L(\\theta | y)}{f(y)} \\] In previous examples (conjugate priors) we were able to do this analytically Numerator - no issue, we specify these distributions. Denominator - Can be difficult or intractable to compute the denominator f(y) ! \\[ f(y) = \\int_{\\theta_1}\\int_{\\theta_2} ... \\int_{\\theta_k}f(\\theta)L(\\theta | y) d\\theta_k ... d\\theta_1 d\\theta_2 \\] Solution? Aapproximate the posterior via simulation! "],["grid-approximaiton.html", "6.2 Grid Approximaiton", " 6.2 Grid Approximaiton Discretized approximation of the posterior. Method of producing samples: Define a grid of possible values of the parameters \\(\\theta\\), Evaluate the numerator at each possible value Obtain a discrete approximation of \\(f(\\theta|y)\\) by normalizing the results (i.e.¬†divide by the sum!) Randomly sample the grid values using the probabilities determined in step 3. "],["beta-binomial-example-grid.html", "6.3 Beta Binomial Example (Grid)", " 6.3 Beta Binomial Example (Grid) \\[ Y|\\pi \\sim Bin(10,\\pi)\\\\ \\pi \\sim Beta(2,2)\\] Observe \\(Y=9\\) successes. # Step 1: Define grid grid_data &lt;- data.frame(pi_grid = seq(from = 0, to = 1, length = 100)) # Step 2: Evaluate numerator grid_data &lt;- grid_data %&gt;% mutate(prior = dbeta(pi_grid, 2, 2), likelihood = dbinom(9, 10, pi_grid)) %&gt;% mutate( unnormalized = prior*likelihood) # Step 3: Normalize! grid_data &lt;- grid_data %&gt;% mutate(posterior = unnormalized/sum(unnormalized)) ggplot(grid_data, aes(x = pi_grid, y = posterior)) + geom_point() + geom_segment(aes(x = pi_grid, xend = pi_grid, y = 0, yend = posterior)) Sample from this posterior (Step 4) set.seed(84735) #BAYES post_sample &lt;- sample_n(grid_data, size = 10000, weight = posterior, replace = TRUE) ggplot(post_sample, aes(x = pi_grid)) + geom_histogram(aes(y = ..density..), color = &quot;white&quot;, binwidth = 0.05) + stat_function(fun = dbeta, args = list(11, 3)) + lims(x = c(0, 1)) ## Warning: Removed 2 rows containing missing values (geom_bar). We can compute any summary statistics from the samples (or from the grid posterior itself!) "],["mcmc.html", "6.4 MCMC", " 6.4 MCMC Curse of dimensionality -&gt; Grid approximation is limited to cases with only a few parameters. Markov Chain Monte Carlo produces a Markov chain of samples to approximate posterior. Markov Chain: \\(\\theta^{(i+1)} \\sim f(\\theta^{(i+1)} | \\theta^{(i)}, y)\\) Monte Carlo: Random samples from chain. Samples are not directly from the posterior and are not independent! More on how it works in next chapter. But we can just jump in with rstan "],["beta-binomial-mcmc.html", "6.5 Beta-Binomial (MCMC)", " 6.5 Beta-Binomial (MCMC) # define model in stan language bb_model &lt;- &quot; data { int&lt;lower = 0, upper = 10&gt; Y; } parameters { real&lt;lower = 0, upper = 1&gt; pi; } model { Y ~ binomial(10, pi); pi ~ beta(2, 2); } &quot; # use stan to simulate posterior bb_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), chains = 4, iter = 5000*2, seed = 84735) Uses 4 chains and 10000 samples of which 1/2 are discarded by default for burn-in Result is a stanfit object, which can be used to extract the samples # for examining using view chains &lt;- as.data.frame(as.array(bb_sim, pars = &quot;pi&quot;)) # look at a zoom in of the sample trace mcmc_trace(bb_sim, pars = &quot;pi&quot;, window = c(50,100),size =0.1) Trace shows the samples exploring the parameter space but also illustrates non-zero autocorrelation. We can also plot the resulting distribution of samples (book shows that this is close to beta-binomial expected) mcmc_dens(bb_sim, pars = &quot;pi&quot;) + yaxis_text(TRUE) + ylab(&quot;density&quot;) "],["markov-chain-diagnostics.html", "6.6 Markov chain diagnostics", " 6.6 Markov chain diagnostics Primary tools: Trace plots Effective sample size Autocorrelation R-hat With trace plots, look for good mixing and compare parallel chains. Effective sample size takes into account the correlation between samples. (best if &gt; 10% of actual samples) neff_ratio(bb_sim, pars = c(&quot;pi&quot;)) ## [1] 0.3472187 Autocorrelation measures the correlation between pairs of Markov chain values that are Lag ‚Äústeps‚Äù apart mcmc_acf(bb_sim, pars = &quot;pi&quot;) R-hat is the ratio of the variability between chains to the variability within chains. R-hat &gt; 1.05 is cause for concern. rhat(bb_sim, pars=&quot;pi&quot;) ## [1] 1.000393 "],["summary-2.html", "6.7 Summary", " 6.7 Summary More sophisticated Bayesian models often require approximations Learned about two methods: Grid Approximation (straightforward but limited) MCMC (more flexible) Learned some MCMC diagnostics Next chapter, MCMC under the hood "],["meeting-videos-5.html", "6.8 Meeting Videos", " 6.8 Meeting Videos 6.8.1 Cohort 1 6.8.2 Cohort 2 Meeting chat log 00:11:00 Ron: 6.12 through 6.18 "],["mcmc-under-the-hood.html", "Chapter 7 MCMC under the Hood", " Chapter 7 MCMC under the Hood Learning objectives: Conceptual understanding of how Markov chain algorithms work Explore Metropolis-Hastings algorithm Implement it with the Normal-Normal "],["the-big-idea-12.html", "7.1 The big idea 1/2", " 7.1 The big idea 1/2 We are going to use a Normal-Normal model: \\[ Y|\\mu \\sim Norm(\\mu, 0.75^2)\\] \\[ \\mu \\sim Norm(0, 1^2) \\] Observed outcome 6.25: \\[ \\mu|(Y = 6.25) \\sim Norm(4, 0.6^2)\\] Main idea: chain need to spend more time around \\(\\mu\\) value. Remember \\(\\mu^{i+1}\\) is dependant of \\(\\mu^{i}\\). How are we going to visit every part of the posterior dustribution: step 1 : propose a random location \\(\\mu&#39;\\) (I prefer \\(\\mu_{proposal}\\)) for the nex stop step 2 : Decide whether to: go to the proposed location: \\(\\mu_{proposal} = \\mu^{i+1}\\) stay at the current location: \\(\\mu = \\mu^{i+1}\\) Monte Carlo algorithm: step 1 propose location: draw \\(\\mu\\) from posterior model with \\(pdf \\quad f(\\mu|y)\\) step 2 : Go there "],["the-big-idea-22.html", "7.2 The big idea 2/2", " 7.2 The big idea 2/2 But we are using MCMC to approximate this pdf (so we can‚Äôt sample it). For we are going to use two tricks: We do know that \\(f(u|y = 6.25) \\propto f(\\mu)L(u|y = 6.25)\\) For step 1 we can use an other model or distribution to generate proposals \\[ \\mu_{proposal}|\\mu \\sim Unif(\\mu - w, \\mu, + w) \\] with pdf: \\[ q(\\mu&#39;|\\mu) = \\frac{1}{2 w}\\] This give us a way for doing step1! "],["the-metropolis-hastings-algorithm.html", "7.3 The Metropolis-Hastings algorithm", " 7.3 The Metropolis-Hastings algorithm step 1: ok : \\(q(\\mu_{proposal}|\\mu)\\) step 2: we are going to calculate an acceptance probability: \\[\\alpha = \\{1, \\frac{f(\\mu_{proposal}) L(\\mu_{proposal}|y) q(\\mu|\\mu_{proposal})}{f(\\mu)L(\\mu|y) q(\\mu_{proposal|\\mu})} \\}\\] Because the Uniform proposal model is symmetric: \\[ q(\\mu_{proposal}|\\mu) = q(\\mu|\\mu_{proposal})\\] Then after multiplying by \\(f(y)\\) we have now: \\[ \\alpha = min\\{1, \\frac{f(\\mu_{proposal}|y)}{f(\\mu|y)} \\} \\] This ration is equivalent to the unnormalized posterior. Two scnearios: Scenario 1: \\(f(\\mu_{proposal}|y) \\geq f(\\mu|y)\\) -&gt; \\(\\alpha = 1\\) we are moving Scenario 2: $f(_{proposal}|y) &lt; f(|y) $ then we move according to the probability \\(\\alpha\\) one_mh_iteration &lt;- function(w, current){ # STEP 1: Propose the next chain location proposal &lt;- runif(1, min = current - w, max = current + w) # STEP 2: Decide whether or not to go there proposal_plaus &lt;- dnorm(proposal, 0, 1) * dnorm(6.25, proposal, 0.75) current_plaus &lt;- dnorm(current, 0, 1) * dnorm(6.25, current, 0.75) alpha &lt;- min(1, proposal_plaus / current_plaus) next_stop &lt;- sample(c(proposal, current), size = 1, prob = c(alpha, 1-alpha)) # Return the results return(data.frame(proposal, alpha, next_stop)) } set.seed(8) one_mh_iteration(w = 1, current = 3) ## proposal alpha next_stop ## 1 2.93259 0.8240205 2.93259 "],["implementing-the-metropolis-hastings.html", "7.4 Implementing the Metropolis-Hastings", " 7.4 Implementing the Metropolis-Hastings mh_tour &lt;- function(N, w){ # 1. Start the chain at location 3 current &lt;- 3 # 2. Initialize the simulation mu &lt;- rep(0, N) # 3. Simulate N Markov chain stops for(i in 1:N){ # Simulate one iteration sim &lt;- one_mh_iteration(w = w, current = current) # Record next location mu[i] &lt;- sim$next_stop # Reset the current location current &lt;- sim$next_stop } # 4. Return the chain locations return(data.frame(iteration = c(1:N), mu)) } library(ggplot2) set.seed(84735) mh_simulation_1 &lt;- mh_tour(N = 5000, w = 1) ggplot(mh_simulation_1, aes(x = iteration, y = mu)) + geom_line() ggplot(mh_simulation_1, aes(x = mu)) + geom_histogram(aes(y = ..density..), color = &quot;white&quot;, bins = 20) + stat_function(fun = dnorm, args = list(4,0.6), color = &quot;blue&quot;) ## Tuning the Metropolis-Hastings algorithm 7.4.1 Quiz! w = 0.01 or w = 100, or w = 1 "],["a-beta-binomial-example.html", "7.5 A Beta-Binomial example", " 7.5 A Beta-Binomial example 1 success in 2 trials \\[ Y|\\pi = bin(2, \\pi) \\] \\[ \\pi = Beta(2,3) \\] We are still playing ‚Äúpretend‚Äù We are moving for step 1 to an Uniform to a Beta model because we want \\(\\pi\\) to be [0,1]. And we will draw every step from this Beta model. -&gt; change in step 1 \\[\\alpha = min \\{1, \\frac{f(\\pi_{proposal}|y)q(\\pi)}{f(\\pi|y) q(\\pi_{proposal})} \\} \\] one_iteration &lt;- function(a, b, current){ # STEP 1: Propose the next chain location proposal &lt;- rbeta(1, a, b) # STEP 2: Decide whether or not to go there proposal_plaus &lt;- dbeta(proposal, 2, 3) * dbinom(1, 2, proposal) proposal_q &lt;- dbeta(proposal, a, b) # &lt;- new current_plaus &lt;- dbeta(current, 2, 3) * dbinom(1, 2, current) current_q &lt;- dbeta(current, a, b) # &lt;- new alpha &lt;- min(1, proposal_plaus / current_plaus * current_q / proposal_q) next_stop &lt;- sample(c(proposal, current), size = 1, prob = c(alpha, 1-alpha)) return(data.frame(proposal, alpha, next_stop)) } betabin_tour &lt;- function(N, a, b){ # 1. Start the chain at location 0.5 current &lt;- 0.5 # 2. Initialize the simulation pi &lt;- rep(0, N) # 3. Simulate N Markov chain stops for(i in 1:N){ # Simulate one iteration sim &lt;- one_iteration(a = a, b = b, current = current) # Record next location pi[i] &lt;- sim$next_stop # Reset the current location current &lt;- sim$next_stop } # 4. Return the chain locations return(data.frame(iteration = c(1:N), pi)) } "],["why-the-algorithm-works.html", "7.6 Why the algorithm works", " 7.6 Why the algorithm works If the algorithm works : \\[\\frac{\\mu \\rightarrow \\mu_{proposa}}{\\mu_{proposal} \\rightarrow \\mu} = \\frac{ f(\\mu_{proposal}|y)}{f(\\mu|y)}\\] "],["chapter-summary.html", "7.7 Chapter summary", " 7.7 Chapter summary Step 1 Propose a new chain location by drawing from a proposal pdf which is perhaps dependent upon the current location Determine whether accept the proposal: depends on how favorable its posterior plausibility is relative to the posterior plausibility of the current location "],["meeting-videos-6.html", "7.8 Meeting Videos", " 7.8 Meeting Videos 7.8.1 Cohort 1 Meeting chat log 00:00:12 Federica Gazzelloni: Hello! 00:58:29 olivier leroy: while (count_accept &lt; M) { count_draw &lt;- count_draw + 1 un_runif &lt;- runif(1) k &lt;- rbinom(prob = un_runif, size = 10, n = 1) if(k == kobs) { count_accept &lt;- count_accept + 1 post[count_accept] &lt;- un_runif } } 01:04:14 olivier leroy: https://www.youtube.com/watch?v=Qqz5AJjyugM&amp;list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&amp;index=8&amp;pp=sAQB 7.8.2 Cohort 2 "],["posterior-inference-prediction.html", "Chapter 8 Posterior Inference &amp; Prediction", " Chapter 8 Posterior Inference &amp; Prediction Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-3.html", "8.1 SLIDE 1", " 8.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-7.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log 00:00:57 Olivier‚Äôs iPhone: I am on mobile phone now, I will turn camera later! 01:08:30 Brendan Lam: Learning Bayesian Stats Podcast: https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5jYXB0aXZhdGUuZm0vbGVhcm5iYXllc3N0YXRzLw?sa=X&amp;ved=0CAMQ4aUDahcKEwj45ff4opz5AhUAAAAAHQAAAAAQAQ&amp;hl=en-CA 01:14:30 Brendan Lam: https://errorstatistics.com/ 8.2.2 Cohort 2 "],["simple-normal-regression.html", "Chapter 9 Simple Normal Regression", " Chapter 9 Simple Normal Regression Learning objectives: Building simple linear regression model Understand Bayesian approach to regression Interpret appropriate prior models Simulate posterior model of the regression parameters Utilize simulation result to build posterior understanding of relationship between response (\\(Y\\)) and predictors (\\(X\\)) build posterior predictive models of \\(Y\\) "],["begining-of-unit-3.html", "9.1 Begining of Unit 3!", " 9.1 Begining of Unit 3! Congratulations on our progress! "],["new-terms.html", "9.2 New terms", " 9.2 New terms \\(Y \\rightarrow\\) Response variable \\(X_{1}, X_{2}, ..., X_{p} \\rightarrow\\) Predictors We want to analyze quantitative response : Regression We want to analyze categorical response: Classification In this chapter we will focus on the Normal regression model. Our toy example will come from a bike sharing service. We will try to understand the demand for it service. We want a model of the number of rides/day. Poisson model is not valid here because we do not have an equal mean and variance. Instead we are going to go with a normal model: \\[Y_{i}|\\mu, \\sigma \\overset{ind}{\\sim} N(\\mu, \\sigma^2) \\] \\[ \\mu \\sim N(\\theta, \\tau^2) \\] \\[ \\sigma \\sim some \\; prior \\; model \\] Can‚Äôt we do better with a predictor? Here it will be the temperature in Fahrenheit. "],["building-the-regression-model.html", "9.3 Building the regression model", " 9.3 Building the regression model 9.3.1 Data model We will have n data pairs of bike ridership (\\(Y\\)) and temperature (\\(X\\)) : \\[\\{(Y_{1}, X_{1}), (Y_{2}, X_{2}), ..., (Y_{n}, X_{n}) \\}\\] Here prior knowledge suggest positive linear relationship between ridership and temperature: the warmer it is, the more likely people are using bike share service. We are now moving away from the global mean (\\(\\mu\\)) to local mean (\\(\\mu_{i}\\), where \\(i\\) is one day). If the relationship is linear : \\[ \\mu_{i} = \\beta_{0} + \\beta_{1}X_{i} \\] \\(\\beta_{0}\\) is the intercept coefficent but it is hard to interpret (would you rent bike when it is 0 degree F?) \\(\\beta_{1}\\) is the Temperature coefficient it indicates the typical change in ridership for every one unit increase in temperature. In case we have just one quantitative predictor it is called the slope. We can plunk this assumption in our model : \\[ Y_{i}| \\beta_{0}, \\beta_{1}, \\sigma \\overset{ind}{\\sim} N(\\mu_{i}, \\sigma^2) \\; with \\quad \\mu_{i} = \\beta_{0} + \\beta_{1}X_{i} \\] As you can see \\(\\sigma\\) is now about variability from the local mean 9.3.1.1 Normal regression assumptions Structure of the data: accounting for \\(X\\), \\(Y\\) for one day is independent of an other day Structure of the relationship: Y can be written as a linear function of predictor X : \\(\\mu = \\beta_{0} + \\beta_{1}X\\) Structure of the variability: at any value of X, Y will vary normaly around \\(\\mu\\) with a consistent standard deviation \\(\\sigma\\) 9.3.2 Specifying the priors 9.3.2.1 Quiz! What are our parameters ? Results \\[\\beta_{0}, \\beta_{1}, \\sigma\\] First assumption our parameters are independent \\[ \\beta_{0} \\sim N(m_{0}, s^2_{0} ) \\] \\[ \\beta_{1} \\sim N(m_{1}, s^2_{1} ) \\] \\(m_{0}, m_{1}, s_{0}, s_{1}\\) are parameters of parameters so they are : hyperparameters \\[\\sigma \\sim Exp(l)\\] 9.3.3 Putting it all together \\[ Y_{i}| \\beta_{0}, \\beta_{1}, \\sigma \\overset{ind}{\\sim} N(\\mu_{i}, \\sigma^2) \\; with \\quad \\mu_{i} = \\beta_{0} + \\beta_{1}X_{i} \\] \\[ \\beta_{0} \\sim N(m_{0}, s^2_{0} ) \\] \\[ \\beta_{1} \\sim N(m_{1}, s^2_{1} ) \\] \\[\\sigma \\sim Exp(l)\\] Model building one step at a time ! Y is discrete or continuous \\(\\rightarrow\\) appropriate model for data Rewrite the mean of Y as a function of predictors X (e.g.¬†\\(\\mu = \\beta_0 + \\beta_1 X\\)) Identify unknown parameters in your model Note the values these parameters might take \\(\\rightarrow\\) Identify appropriate priors "],["tuning-prior-models-for-regression-parameters.html", "9.4 Tuning prior models for regression parameters", " 9.4 Tuning prior models for regression parameters An average temperature day (65, 70 degree F) in DC: 3000-7000 riders with around 5000 For every one degree increase you get about +100 riders \\(\\pm\\) 80 At any given Temperature daily ridership vary with a sd of 1250 rides We will work with centered data \\(\\beta_{0} \\rightarrow \\beta_{0c}\\) because : it is easier to interpret and specify in this example this is what rstanarm uses \\[ Y_{i}| \\beta_{0}, \\beta_{1}, \\sigma \\overset{ind}{\\sim} N(\\mu_{i}, \\sigma^2) \\; with \\quad \\mu_{i} = \\beta_{0} + \\beta_{1}X_{i} \\] \\[ \\beta_{0c} \\sim N(5000, 1000^2 ) \\tag{a.}\\] \\[ \\beta_{1} \\sim N(100, 40^2 \\tag{b.}) \\] \\[\\sigma \\sim Exp(0.0008). \\tag{c.}\\] The only hard part was using an equation from chapter 5: \\[E(\\sigma) = \\frac{1}{l} = 1250\\] It is good to simulate this prior and see what they look like but we will do that in the part about using default rstanarm priors later. "],["posterior-simulatiion.html", "9.5 Posterior simulatiion", " 9.5 Posterior simulatiion Now we want to update our prior with data to get a posterior simulation! # Load and plot data library(bayesrules);library(ggplot2) data(bikes) ggplot(bikes, aes(x = temp_feel, y = rides)) + geom_point(size = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; I am saving you from the triple integrals in the denominator of p 220 and we are jumping directly to MCMC! We are going to use rstanarm : Rstan + arm = applied regression models 9.5.1 Simulation via rstanarm bike_model &lt;- rstanarm::stan_glm( # data information rides ~ temp_feel, # &lt;- formula syntax (as used by lm, glm etc) data = bikes, family = gaussian, # &lt;- we assume normal data # Priors prior_intercept = normal(5000, 1000), # centered intercept prior = normal(100, 40), prior_aux = exponential(0.0008), #notice the aux for auxiliary more after # MCMC information chains = 4, iter = 5000*2, seed = 84735) 9.5.2 Simulation directly with rstan # STEP 1: DEFINE the model stan_bike_model &lt;- &quot; data { int&lt;lower = 0&gt; n; vector[n] Y; vector[n] X; } parameters { real beta0; real beta1; real&lt;lower = 0&gt; sigma; } model { Y ~ normal(beta0 + beta1 * X, sigma); beta0 ~ normal(-2000, 1000); beta1 ~ normal(100, 40); sigma ~ exponential(0.0008); } &quot; # STEP 2: SIMULATE the posterior stan_bike_sim &lt;- rstan::stan(model_code = stan_bike_model, # data is structured a bit differently data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel), # same MCMC chains = 4, iter = 5000*2, seed = 84735) The model will return 5000 x nb of chains simulation of our parameters. Rstanarm will change their name: \\(\\beta_{0} \\rightarrow (intercept)\\) (Note that this is \\(\\beta_{0}\\) even though prior is specified for \\(\\beta_{0c}\\)) \\(\\beta_{1} \\rightarrow temp\\_feel\\) But \\(\\sigma\\) stay sigma. We need to check if the simulation went well for that we can? Answer: [X] Check the effective sample size: neff_ratio [X] Compute rhat [X] Examine trace plot [X] Overlay the density plots of each chains "],["interpreting-the-posterior.html", "9.6 Interpreting the posterior", " 9.6 Interpreting the posterior What we have is samples of each of the parameters. # Posterior summary statistics broom.mixed::tidy(bike_model, # it take our output from rstanarm::stan_glm effects = c(&quot;fixed&quot;, &quot;aux&quot;), # fixed is regression coef et aux is auxiliary ie sigma conf.int = TRUE, conf.level = 0.80) # A tibble: 4 x 5 term estimate std.error conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -2194. 362. -2656. -1732. 2 temp_feel 82.2 5.15 75.6 88.8 3 sigma 1281. 40.7 1231. 1336. 4 mean_PPD 3487. 80.4 3385. 3591. Here we can get the posterior median relationship : \\[ -2194.24 + 82.16X \\] If we want a bigger picture: # Store the 4 chains for each parameter in 1 data frame bike_model_df &lt;- as.data.frame(bike_model) # Check it out nrow(bike_model_df) [1] 20000 head(bike_model_df, 3) (Intercept) temp_feel sigma 1 -2657 88.16 1323 2 -2188 83.01 1323 3 -1984 81.54 1363 # and use add_fitted_draws() from tidybayes # 50 simulated model lines bikes %&gt;% tidybayes::add_fitted_draws(bike_model, n = 50) %&gt;% ggplot(aes(x = temp_feel, y = rides)) + geom_line(aes(y = .value, group = .draw), alpha = 0.15) + geom_point(data = bikes, size = 0.05) # I did not evaluate the code here Figure 9.7 9.6.0.1 Quiz! Do we have ample posterior evidence that there‚Äôs a positive association between ridership and temperature ? Answer: Visual evidence : 50 or more posterior scenarios that display positive relationship Numerical evidence from CI: 80% CI for \\(\\beta_{1}\\) range from 75.6 to 88.8 Numerical evidence from posterior probability # Tabulate the beta_1 values that exceed 0 bike_model_df %&gt;% mutate(exceeds_0 = temp_feel &gt; 0) %&gt;% tabyl(exceeds_0) # resuly exceeds_0 n percent TRUE 20000 1 "],["posterior-prediction.html", "9.7 Posterior prediction", " 9.7 Posterior prediction 9.7.1 Quiz! Suppose a weather report indicates that tomorrow will be a 75-degree day in D.C. What‚Äôs your posterior guess of the number of riders that Capital Bikeshare should anticipate? Answer: One option is \\[ -2194.24 + 82.16 * 75 = 3967.76 \\] But this does not take into account : Sampling variability Posterior variability The posterior predictive model takes into account both kinds of variability. We can approximate this posterior predictive model with our 20 000 samples of parameters. 9.7.2 Building a posterior predictive model # Predict rides for each parameter set in the chain set.seed(84735) predict_75 &lt;- bike_model_df %&gt;% mutate(mu = `(Intercept)` + temp_feel*75, # &lt;- our 75 degree y_new = rnorm(20000, mean = mu, sd = sigma)) # &lt;- sampling var. head(predict_75, 3) (Intercept) temp_feel sigma mu y_new 1 -2657 88.16 1323 3955 4838 2 -2188 83.01 1323 4038 3874 3 -1984 81.54 1363 4132 5196 Interesting point is mu (\\(\\mu\\)) vs.¬†y_new (\\(Y_{new}\\)). # Construct 80% posterior credible intervals predict_75 %&gt;% summarize(lower_mu = quantile(mu, 0.025), upper_mu = quantile(mu, 0.975), lower_new = quantile(y_new, 0.025), upper_new = quantile(y_new, 0.975)) lower_mu upper_mu lower_new upper_new 1 3843 4095 1500 6482 \\(\\mu\\) is average in readership for 75 degree \\(Y_new\\) is for a specific day (with 75 degree) =&gt; More accuracy in predicting an average than an unique point! 9.7.3 Posterior with rstanarm We have done it from ‚Äúscratch‚Äù but we can use rstanarm::posterior_predict() # Simulate a set of predictions set.seed(84735) shortcut_prediction &lt;- posterior_predict(bike_model, newdata = data.frame(temp_feel = 75)) "],["sequential-regression-modeling.html", "9.8 Sequential regression modeling", " 9.8 Sequential regression modeling We can have our data that come in sequences: phase_1 &lt;- bikes[1:30, ] phase_2 &lt;- bikes[1:60, ] phase_3 &lt;- bikes Bayes rules !figure 9.13 "],["using-default-rstanarm-priors.html", "9.9 Using default rstanarm priors", " 9.9 Using default rstanarm priors Authors recommend using the default prior from rstanarm: bike_model_default &lt;- rstanarm::stan_glm( rides ~ temp_feel, data = bikes, family = gaussian, # here very specific prior on sd prior_intercept = normal(5000, 2.5, autoscale = TRUE), # &lt;- see autoscale arg. prior = normal(0, 2.5, autoscale = TRUE), prior_aux = exponential(1, autoscale = TRUE), chains = 4, iter = 5000*2, seed = 84735) prior_summary(bike_model_default) Priors for model &#39;bike_model_default&#39; ------ Intercept (after predictors centered) Specified prior: ~ normal(location = 5000, scale = 2.5) Adjusted prior: ~ normal(location = 5000, scale = 3937) Coefficients Specified prior: ~ normal(location = 0, scale = 2.5) Adjusted prior: ~ normal(location = 0, scale = 351) Auxiliary (sigma) Specified prior: ~ exponential(rate = 1) Adjusted prior: ~ exponential(rate = 0.00064) ------ See help(&#39;prior_summary.stanreg&#39;) for more details It uses weakly informative priors using the scale of the data. Figure 9.1: Fig 9.5 and 9.14 from Bayes Rules! "],["you-are-not-done-yet.html", "9.10 You are not done yet!", " 9.10 You are not done yet! We have learned just enough to be dangerous! Review next chapter on evaluating the model before applying this new tool! "],["summary-3.html", "9.11 Summary", " 9.11 Summary Built a simple Bayesian Normal regression with response and predictor quantitative First example of a case where Markov Chain Monte Carlo simulation was really needed! Used simulated samples to summarize our posterior understanding of the relationship between response and predictor. Used simulated samples for posterior prediction. "],["resources.html", "9.12 Resources:", " 9.12 Resources: Rstanarm vignette "],["meeting-videos-8.html", "9.13 Meeting Videos", " 9.13 Meeting Videos 9.13.1 Cohort 1 9.13.2 Cohort 2 Meeting chat log LOG "],["evaluating-regression-models.html", "Chapter 10 Evaluating Regression Models", " Chapter 10 Evaluating Regression Models Learning objectives: Determine whether a model is fair Determine how wrong a model is Determine our model‚Äôs posterior predictive accuracy "],["more-question-to-ask.html", "10.1 More question to ask", " 10.1 More question to ask When we look at the Bayesian model results, it might be important to investigate a bit more about: How was the data collected? By whom and for what purpose was the data collected? How might the results of the analysis, or the data collection itself, impact individuals and society? What biases might be baked into this analysis? (#fig:10.1)Credits: https://godcgo.com/a-bike-friendly-washington-dc/ "],["verifying-normal-regression-assumptions.html", "10.2 Verifying Normal regression assumptions", " 10.2 Verifying Normal regression assumptions Assumption 1: Structure of the data independency Assumption 2: Structure of the relationship linearity Assumption 3 : Structure of the variability normality "],["our-model-case-is-made-of.html", "10.3 Our model-case is made of:", " 10.3 Our model-case is made of: number of Capital Bikeshare rides: \\(Y_{i}\\) temperature on day \\(i\\): \\(X_{i}\\) (#fig:10.2)Credits: dezeen.com Starting from a Regression Model: \\[ Y_{i} = \\beta_{0} + \\beta_{1}X_{i} \\] We look for an approximation of the real mean value: estimate mean value \\[Y_{i} \\approx\\mu_{i}\\] \\[ \\mu_{i} = \\beta_{0} + \\beta_{1}X_{i} \\] ‚Äú‚Ä¶To turn this into a Bayesian model, we must incorporate prior models for each of the unknown regression parameters‚Ä¶(ct.9.1.2)‚Äù with this model parameters: \\[ Y_{i}| \\beta_{0}, \\beta_{1}, \\sigma \\overset{ind}{\\sim} N(\\mu_{i}, \\sigma^2) \\; with \\quad \\mu_{i} = \\beta_{0} + \\beta_{1}X_{i} \\] we expect some good results as we consider 500 daily observations within the two-year period. The response variable ridership \\(Y\\) is likely to be correlated over time with other features such as temperature \\(X\\). ‚Äú‚Ä¶today‚Äôs ridership likely tells us something about tomorrow‚Äôs ridership. Yet much of this correlation, or dependence, can be explained by the time of year and features associated with the time of year‚Ä¶‚Äù ‚Äú‚Ä¶knowing the temperature on two subsequent days may very well ‚Äúcancel out‚Äù the time correlation in their ridership data‚Ä¶‚Äù We are tempted to conclude: the temperatures in one location are independent of those in neighboring locations, the temperatures in one month don‚Äôt tell us about the next. (#fig:10.3)Assumption1 It‚Äôs reasonable to assume that, in light of the temperature \\(X\\), ridership data \\(Y\\) is independent from day to day. We are looking for a centered value of the intercept: \\[ \\beta_{0c} \\sim N(m_{0}, s^2_{0})= N(5000, 1000^2 ) \\tag{a.}\\] \\[ \\beta_{1} \\sim N(m_{1}, s^2_{1}) = N(100, 40^2 \\tag{b.}) \\] \\[ \\sigma\\sim Exp(l) = Exp(0.0008 \\tag{c.}) \\] data(bikes) ## date rides temp_feel ## 1 2011-01-01 654 64.72625 ## 2 2011-01-03 1229 49.04645 ## 3 2011-01-04 1454 51.09098 ## 4 2011-01-05 1518 52.63430 ## 5 2011-01-07 1362 50.79551 ## 6 2011-01-08 891 46.60286 To evaluate assumptions 2 and 3 we conduct a posterior predictive check. Our first look was at the relationship between rides and temperature, and so at the consistency of the distribution. (#fig:10.7)Assumption 2 and 3 Given the combined model assumptions reasonable, the posterior model should be able to simulate ridership data very close to the original 500 rides observations. bike_model &lt;- rstanarm::stan_glm( rides ~ temp_feel, data = bikes, family = gaussian, prior_intercept = normal(5000, 1000), prior = normal(100, 40), prior_aux = exponential(0.0008), chains = 4, iter = 5000*2, seed = 84735, # suppress the output refresh=0) bike_model_df &lt;- as.data.frame(bike_model) first_set &lt;- head(bike_model_df, 1) first_set ## (Intercept) temp_feel sigma ## 1 -2040.536 80.44231 1280.101 beta_0 &lt;- first_set$`(Intercept)` beta_1 &lt;- first_set$temp_feel sigma &lt;- first_set$sigma set.seed(84735) one_simulation &lt;- bikes %&gt;% mutate(mu = beta_0 + beta_1 * temp_feel, simulated_rides = rnorm(500, mean = mu, sd = sigma)) %&gt;% select(temp_feel, rides, simulated_rides) one_simulation%&gt;%head ## temp_feel rides simulated_rides ## 1 64.72625 654 4020.319 ## 2 49.04645 1229 1746.346 ## 3 51.09098 1454 3068.930 ## 4 52.63430 1518 2496.768 ## 5 50.79551 1362 3065.187 ## 6 46.60286 891 3735.023 ggplot(one_simulation, aes(x = simulated_rides)) + geom_density(color = &quot;lightblue&quot;) + geom_density(aes(x = rides), color = &quot;darkblue&quot;)+ labs(title=&quot;One posterior simulated dataset of ridership (light blue)\\nalong with the actual observed ridership data (dark blue).&quot;)+ ggthemes::theme_fivethirtyeight()+ theme(plot.title = element_text(size=10)) "],["posterior-predictive-check.html", "10.4 Posterior predictive check", " 10.4 Posterior predictive check Use the pp_check() function from {bayesplot} package included in the {rstanarm} package. It compares the observed outcome variable y to simulated datasets from the posterior predictive distribution. # Examine 50 of the 20000 simulated samples pp_check(bike_model,nreps = 50) + xlab(&quot;rides&quot;)+ labs(title=&quot;50 datasets of ridership simulated from the posterior (light blue)\\nalongside the actual observed ridership data (dark blue)&quot;) So, in general to check the last two assumptions you should: Assume a different data structure Make a transformation "],["how-accurate-are-the-posterior-predictive-models.html", "10.5 How accurate are the posterior predictive models?", " 10.5 How accurate are the posterior predictive models? Three approaches to evaluating predictive quality Posterior predictive summaries median absolute error (MAE) scaled median absolute error within_50 and within_95 set.seed(84735) prediction_summary(bike_model, data = bikes) ## mae mae_scaled within_50 within_95 ## 1 990.4453 0.7709476 0.438 0.966 set.seed(84735) predict_75 &lt;- bike_model_df %&gt;% mutate(mu = `(Intercept)` + temp_feel*75, y_new = rnorm(20000, mean = mu, sd = sigma)) # Plot the posterior predictive model ggplot(predict_75, aes(x = y_new)) + geom_density()+ geom_vline(aes(xintercept = 6228))+ labs(title=&quot;The posterior predictive model of ridership on October 22, 2012,\\na 75-degree day. The actual Y = 6228 riders observed that day are\\nmarked by the vertical line.&quot;)+ ggthemes::theme_fivethirtyeight()+ theme(plot.title = element_text(size=10)) set.seed(84735) predictions &lt;- posterior_predict(bike_model, newdata = bikes) dim(predictions) ## [1] 20000 500 ppc_intervals(bikes$rides, yrep = predictions, x = bikes$temp_feel, prob = 0.5, prob_outer = 0.95)+ labs(title=&quot;The posterior predictive medians (light blue dots),\\n50% prediction intervals (wide, short blue bars),\\nand 95% prediction intervals (narrow, long blue bars)\\nfor each day in the bikes dataset, along with the corresponding\\nobserved data points (dark blue dots).&quot;) Cross-validation To see how well our model generalizes to new data beyond our original sample, we can estimate these properties using cross-validation techniques. Train the model Test the model set.seed(84735) cv_procedure &lt;- prediction_summary_cv(model = bike_model, data = bikes, k = 10) cv_procedure$folds%&gt;%head ## fold mae mae_scaled within_50 within_95 ## 1 1 989.9688 0.7695714 0.46 0.98 ## 2 2 965.4630 0.7432483 0.42 1.00 ## 3 3 949.6831 0.7292722 0.42 0.98 ## 4 4 1018.8814 0.7911418 0.46 0.98 ## 5 5 1161.6688 0.9091497 0.36 0.96 ## 6 6 937.0211 0.7321570 0.46 0.94 cv_procedure$cv ## mae mae_scaled within_50 within_95 ## 1 1029.1 0.8014163 0.422 0.968 All we want is: (#fig:10.21)Two hypothetical posterior predictive pdfs for Y new, the yet unobserved ridership on a new day. The eventual observed value of y new, is represented by a dashed vertical line Expected log-predictive density (ELPD) ELPD measures the average log posterior predictive pdf, across all possible new data points. The higher the ELPD, the better. Higher ELPDs indicate greater posterior predictive accuracy when using our model to predict new data points. The loo() function in the {rstanarm} package utilizes leave-one-out cross-validation to estimate the ELPD of a given model: model_elpd &lt;- loo(bike_model) model_elpd$estimates ## Estimate SE ## elpd_loo -4289.034410 13.11464 ## p_loo 2.501565 0.16400 ## looic 8578.068821 26.22928 "],["improving-posterior-predictive-accuracy.html", "10.6 Improving posterior predictive accuracy", " 10.6 Improving posterior predictive accuracy Collect more data Use different or more predictors "],["how-good-is-the-mcmc-simulation-vs-how-good-is-the-model.html", "10.7 How good is the MCMC simulation vs how good is the model?", " 10.7 How good is the MCMC simulation vs how good is the model? How well our MCMC simulation approximates the model? Does the model fit? are the assumptions reasonable? is the model fair? does it produce good predictions? "],["extra-resources.html", "10.8 Extra resources:", " 10.8 Extra resources: The Impact of Weather Conditions on Capital Bikeshare Trips "],["meeting-videos-9.html", "10.9 Meeting Videos", " 10.9 Meeting Videos 10.9.1 Cohort 1 10.9.2 Cohort 2 Meeting chat log LOG "],["extending-the-normal-regression-model.html", "Chapter 11 Extending the Normal Regression Model", " Chapter 11 Extending the Normal Regression Model Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-4.html", "11.1 SLIDE 1", " 11.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-10.html", "11.2 Meeting Videos", " 11.2 Meeting Videos 11.2.1 Cohort 1 Meeting chat log 00:05:50 olivier: need to relaod! 00:06:02 olivier: but yes quarto seems a bit difficukt 00:08:30 Brendan Lam: HI there! 00:08:33 Brendan Lam: I can&#39;t talk right now 00:08:39 Brendan Lam: But I&#39;ll be here 00:09:01 Federica Gazzelloni: Hello! 00:10:12 Brendan Lam: i think we call them internet cafes 00:10:31 olivier: iinternet taxi 00:12:56 Federica Gazzelloni: hope you find it useful 00:26:10 Brendan Lam: I believe u can do hypothesis testing in Bayesian settings, but the notion of Type I and II error rates aren&#39;t the same? 00:26:27 olivier: called bayes factor no ? unsure 00:26:50 Brendan Lam: Yes^ 00:30:30 olivier: temp3pm ~ . = temp3pm = hundity * wind * pressure 00:30:58 olivier: or temp3pm = humidity + wind etc 00:43:36 Brendan Lam: How many book clubs r you in Federica?? 00:44:10 Federica Gazzelloni: http://www.feat.engineering/ 00:44:12 olivier: R-inla 00:45:12 olivier: do4ds 00:46:06 olivier: https://do4ds.com/ 00:47:14 Brendan Lam: Me too 00:51:44 Brendan Lam: Might be my last one! 00:51:53 Brendan Lam: I have meetings at this time when school starts 00:51:58 Brendan Lam: but will try my best! 00:52:05 Brendan Lam: I might!! 00:52:37 Brendan Lam: Thanks everyone! 00:52:47 Erik Aa: thanks everyone 11.2.2 Cohort 2 Meeting chat log LOG "],["poisson-negative-binomial-regression.html", "Chapter 12 Poisson &amp; Negative Binomial Regression", " Chapter 12 Poisson &amp; Negative Binomial Regression Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-5.html", "12.1 SLIDE 1", " 12.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-11.html", "12.2 Meeting Videos", " 12.2 Meeting Videos 12.2.1 Cohort 1 Meeting chat log 00:13:31 olivier: http://c1.staticflickr.com/8/7199/6867921547_239ce73660.jpg 00:17:00 Will Parbury: https://en.wikipedia.org/wiki/Julia_Child 00:19:52 olivier: 16:9 00:23:18 olivier: I am lagging a b it so I will switch off video 00:27:53 olivier: ln 00:34:40 olivier: r$&gt; exp(-0.03) [1] 0.9704455 00:35:43 olivier: r$&gt; exp(0.03) [1] 1.030455 00:50:38 olivier: c 12.2.2 Cohort 2 Meeting chat log LOG "],["logistic-regression.html", "Chapter 13 Logistic Regression", " Chapter 13 Logistic Regression Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-6.html", "13.1 SLIDE 1", " 13.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-12.html", "13.2 Meeting Videos", " 13.2 Meeting Videos 13.2.1 Cohort 1 Meeting chat log LOG 13.2.2 Cohort 2 Meeting chat log LOG "],["naive-bayes-classification.html", "Chapter 14 Naive Bayes Classification", " Chapter 14 Naive Bayes Classification Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-7.html", "14.1 SLIDE 1", " 14.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-13.html", "14.2 Meeting Videos", " 14.2 Meeting Videos 14.2.1 Cohort 1 Meeting chat log LOG 14.2.2 Cohort 2 Meeting chat log LOG "],["hierarchical-models-are-exciting.html", "Chapter 15 Hierarchical Models are Exciting", " Chapter 15 Hierarchical Models are Exciting Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-8.html", "15.1 SLIDE 1", " 15.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-14.html", "15.2 Meeting Videos", " 15.2 Meeting Videos 15.2.1 Cohort 1 Meeting chat log LOG 15.2.2 Cohort 2 Meeting chat log LOG "],["normal-hierarchical-models-without-predictors.html", "Chapter 16 (Normal) Hierarchical Models without Predictors", " Chapter 16 (Normal) Hierarchical Models without Predictors Learning objectives: Build a hierarchical model of variabl \\(Y\\) with no predictors \\(X\\) Simulate and analyze this hierarchical model with rstanarm Utilize hierarchical models for predicting \\(Y\\) "],["data-set.html", "16.1 Data Set!", " 16.1 Data Set! data(spotify, package = &quot;bayesrules&quot;) We are going to use a subset Spotify data set from #TidyTuesday: 44 artists -&gt; 350 songs spotify &lt;- spotify |&gt; select(artist, title, popularity) |&gt; mutate(artist = fct_reorder(artist, popularity, .fun = &#39;mean&#39;)) table(spotify$artist) |&gt; hist(xlab = &quot;Songs/artist&quot;, col = 4) We are going to illustrate the 3 approaches seen in chapter 15: Complete pooling No pooling Partial pooling Btw my top 3 most listening song/group from last week are: Dina Summer - Passion (cover) The Do - Anita No Purrrple cat - stream "],["complete-pooled-model.html", "16.2 Complete pooled model", " 16.2 Complete pooled model Notations: \\(j\\) will indicate artist, \\(j \\in {1, 2 ..., 44}\\) \\(i\\) will indicate song for artist \\(j\\) \\(n_j\\) Number of song we have for artist \\(j\\) Example: Mia X, the first artist in our data set, has 4 songs -&gt; \\(n_1 = 4\\) ggplot(spotify, aes(x = popularity)) + geom_density() Even if the distribution is left skewed we will go with a Normal-Normal complete pooled model \\[Y_{ij}|\\mu,\\sigma \\sim N (\\mu, \\sigma¬≤)\\] \\[\\mu \\sim N(50, 52^2)\\] \\[\\sigma \\sim Exp(0.048)\\] \\(\\mu\\) and \\(\\sigma\\) are global parameter: they do not vary by artist: \\(\\mu\\): global mean popularity \\(\\sigma\\) : global standard deviation in popularity from song to song spotify_complete_pooled &lt;- stan_glm( popularity ~ 1, # trick is here \\mu = beta_0 (intercept) with no X data = spotify, family = gaussian, prior_intercept = normal(50, 2.5, # I do not understand 2.5 autoscale = TRUE), prior_aux = exponential(1, autoscale = TRUE), chains = 4, iter = 5000*2, seed = 84735) complete_summary &lt;- tidy(spotify_complete_pooled, effects = c(&quot;fixed&quot;, &quot;aux&quot;), conf.int = TRUE, conf.level = 0.80) complete_summary ## # A tibble: 3 √ó 5 ## term estimate std.error conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 58.4 1.10 57.0 59.8 ## 2 sigma 20.7 0.776 19.7 21.7 ## 3 mean_PPD 58.4 1.57 56.4 60.4 16.2.1 Quiz!! 3 artist: Mia X, artist with the lowest mean popularity in our data set Beyonc√©, artist with nearly the highest mean popularity in our data set Mohsen Beats, an artist not in out data set Using complete pooled model, what would be the approximate posterior predictive mean for a new song from this 3 artists? artist_means &lt;- spotify |&gt; group_by(artist) |&gt; summarize(count = n(), popularity = mean(popularity)) set.seed(84735) predictions_complete &lt;- posterior_predict(spotify_complete_pooled, newdata = artist_means) ppc_intervals(artist_means$popularity, yrep = predictions_complete, prob_outer = 0.80) + ggplot2::scale_x_continuous(labels = artist_means$artist, breaks = 1:nrow(artist_means)) + xaxis_text(angle = 90, hjust = 1) "],["no-pooled-model.html", "16.3 No pooled model", " 16.3 No pooled model ggplot(spotify, aes(x = popularity, group = artist)) + geom_density() Key points: popularity can differ from one artist to an other some artist have a ‚Äústable‚Äù popularity across their song and some not Let change our model to reflect that: \\[Y_{ij}|\\mu_j, \\sigma \\sim N(\\mu_{j}, \\sigma^2 ) \\] \\(\\mu_{j}\\) : mean song popularity for artist \\(j\\) \\(\\sigma\\) : standard deviation in popularity from song to song within each artist spotify_no_pooled &lt;- stan_glm( popularity ~ artist - 1, data = spotify, family = gaussian, prior = normal(50, 2.5, autoscale = TRUE), prior_aux = exponential(1, autoscale = TRUE), chains = 4, iter = 5000*2, seed = 84735) 16.3.1 Same Quiz but with no pooling!! 3 artist: Mia X, artist with the lowest mean popularity in our data set Beyonc√©, artist with nearly the highest mean popularity in our data set Mohsen Beats, an artist not in out data set set.seed(84735) predictions_no &lt;- posterior_predict( spotify_no_pooled, newdata = artist_means) # Plot the posterior predictive intervals ppc_intervals(artist_means$popularity, yrep = predictions_no, prob_outer = 0.80) + ggplot2::scale_x_continuous(labels = artist_means$artist, breaks = 1:nrow(artist_means)) + xaxis_text(angle = 90, hjust = 1) Two drawbacks: Ignoring other artist when modeling for one specific artist (what happens when fewer data point) If we assume no other artists help us understanding popularity of a specific artist we can not generalize to artist outside of our data set. "],["building-the-hierarchical-model.html", "16.4 Building the hierarchical model", " 16.4 Building the hierarchical model 16.4.1 The hierarchy Layer 1: \\(Y_{ij} | \\mu_j , \\sigma_y\\) how song popularity varies WITHIN artist \\(j\\) Layer 2: \\(\\mu_{j}|\\mu, \\sigma_\\mu\\) how typical popularity \\(\\mu_{j}\\) varies BETWEEN artists Layer 3: \\(\\mu, \\sigma_y, \\sigma_{\\mu}\\) prior models for shared global parameters (order do not necessarily matter) Layer 1: \\[Y_{ij}|\\mu_j, \\sigma_j \\sim N(\\mu_j, \\sigma_y^2)\\] \\(\\mu_j\\) mean song popularity for artist j \\(\\sigma_y\\) within group variability sd in popularity from song to song within each artist -&gt; if we stop here we have a ‚Äúno pooled‚Äù Layer 2: \\[\\mu_{j}|\\mu_{j}. \\sigma_\\mu \\overset{ind}{\\sim} N(\\mu, \\sigma_\\mu^2)\\] \\(\\mu\\) global average: the means popularity ratings for the most average artist \\(\\sigma_{u}\\) \\(between-group variability\\), the standard deviation in mean popularity \\(Œºj\\) from artist to artist. #Normal is not too bad ggplot(artist_means, aes(x = popularity)) + geom_density() Layer 3 : Priors \\[\\mu \\sim N(50, 52^2)\\] (this 52 ???) \\[\\sigma_y \\sim Exp(0.048) \\] \\[ \\sigma \\sim Exp(1)\\] @realHollanders This is a one way analyis of variance (ANOVA) An other way to think about it: \\[Y_{ij}|\\mu_j, \\sigma_j \\sim N(\\mu_j, \\sigma_y^2) \\quad with \\quad \\mu_j = \\mu +b_j\\] \\[b_j | \\sigma_\\mu \\overset{ind}{\\sim} N(0, \\sigma_\\mu^2) \\] Example: if \\(\\mu\\) = 55 and \\(\\mu_j\\) = 65 \\(b_j\\) = 10 16.4.2 within- vs -between-group variability Before we analyse just one source of variability (the individual level), now we have two sources (\\(\\sigma_\\mu, \\sigma_y\\)). The first one is the sqrt(variance) within the group (song of an artist) and the second is the sqrt(variance) between group. The total variance is : \\[Var(Y_{ij} = \\sigma¬≤_y + \\sigma^2_u) \\] Other way of thinking about is: \\(\\frac{\\sigma^2_y}{\\sigma^2_\\mu + \\sigma^2_y}\\) proportion of total variance explained by difference within each group \\(\\frac{\\sigma^2_\\mu}{\\sigma^2_\\mu + \\sigma^2_y}\\) proportion of total variance explained by difference between groups You have correlation in song popularity of the same artist (within group). And assuming each groups are independant we get: \\[Cor(Y_{ij}, Y_{kj}) = \\frac{\\sigma^2_\\mu}{\\sigma^2_\\mu + \\sigma^2_y}\\] ## Posterior analysis 16.4.3 Posterior simulation 47 parameters: 44 artists specific parameters (\\(\\mu_{j}\\)) 3 global parameters (\\(\\mu, \\sigma_j, sigma_\\mu\\)) spotify_hierarchical &lt;- stan_glmer( popularity ~ (1 | artist), # this is the part that tell that artist is a group not a predictor data = spotify, family = gaussian, prior_intercept = normal(50, 2.5, autoscale = TRUE), prior_aux = exponential(1, autoscale = TRUE), prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1), # stuff we will learn chapter 17 suppose to be equivalent to Exp(1) chains = 4, iter = 5000*2, seed = 84735) # Confirm the prior tunings prior_summary(spotify_hierarchical) ## Priors for model &#39;spotify_hierarchical&#39; ## ------ ## Intercept (after predictors centered) ## Specified prior: ## ~ normal(location = 50, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 50, scale = 52) ## ## Auxiliary (sigma) ## Specified prior: ## ~ exponential(rate = 1) ## Adjusted prior: ## ~ exponential(rate = 0.048) ## ## Covariance ## ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details We need to check the success of MCMC (saving you time here) bayesplot::pp_check(spotify_hierarchical) + xlab(&quot;popularity&quot;) Let inspect our simulation: spotify_hierarchical_df &lt;- as.data.frame(spotify_hierarchical) dim(spotify_hierarchical_df) ## [1] 20000 47 16.4.4 Posterior analysis of global parameters \\(\\mu = (intercept)\\) \\(\\sigma_y = sigma\\) \\(\\sigma_\\mu¬≤ = Sigma[artist:(intercep),(Intercept)]\\) Attention! here this is the variance tidy(spotify_hierarchical, effects = &quot;fixed&quot; # for getting global , conf.int = TRUE, conf.level = 0.80) ## # A tibble: 1 √ó 5 ## term estimate std.error conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 52.5 2.41 49.3 55.6 tidy(spotify_hierarchical, effects = &quot;ran_pars&quot;) # PARameters and RANdomness or variability) ## # A tibble: 2 √ó 3 ## term group estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sd_(Intercept).artist artist 15.2 ## 2 sd_Observation.Residual Residual 14.0 An other way: 15.1^2 / (15.1^2 + 14.0^2) # sigma_mu^2 / sigma_mu^2 + sigma_y^2 ## [1] 0.5377468 14.0^2 / (15.1^2 + 14.0^2) # sigma_y^2 / sigma_mu^2 + sigma_y^2 ## [1] 0.4622532 16.4.5 posterior analysis of group specific If you recall that \\(\\mu_j = \\mu + b_{j}\\) We have \\(\\mu\\) and \\(b_j\\) (check spotify_hierarchical_df) # RANdom Values artist_summary &lt;- tidy(spotify_hierarchical, effects = &quot;ran_vals&quot; , conf.int = TRUE, conf.level = 0.80) # Check out the results for the first &amp; last 2 artists # 80% intervall # this produce a summary artist_summary %&gt;% select(level, conf.low, conf.high) %&gt;% slice(1:2, 43:44) ## # A tibble: 4 √ó 3 ## level conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mia_X -40.8 -23.2 ## 2 Chris_Goldarg -39.4 -26.9 ## 3 Lil_Skies 11.1 30.3 ## 4 Camilo 19.4 32.4 dim(artist_summary) ## [1] 44 7 Other way: combining simulations to simulate posterior of \\(\\mu_j\\) \\[\\mu_j = \\mu + b_{j} = (Intercept) + b[(Intercept) \\quad artist:j]\\] artist_chains &lt;- spotify_hierarchical |&gt; spread_draws(`(Intercept)`, b[,artist]) |&gt; mutate(mu_j = `(Intercept)` + b) ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## Please use `gather()` instead. dim(artist_chains) ## [1] 880000 7 artist_chains |&gt; select(artist, `(Intercept)`, b, mu_j) |&gt; head(4) ## # A tibble: 4 √ó 4 ## # Groups: artist [4] ## artist `(Intercept)` b mu_j ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 artist:Alok 45.7 14.9 60.6 ## 2 artist:Atlas_Genius 45.7 3.17 48.9 ## 3 artist:Au/Ra 45.7 10.4 56.1 ## 4 artist:Beyonc√© 45.7 27.6 73.4 # Get posterior summaries for mu_j artist_summary_scaled &lt;- artist_chains |&gt; select(-`(Intercept)`, -b) |&gt; mean_qi(.width = 0.80) |&gt; mutate(artist = fct_reorder(artist, mu_j)) # Check out the results artist_summary_scaled |&gt; select(artist, mu_j, .lower, .upper) |&gt; head(4) ## # A tibble: 4 √ó 4 ## artist mu_j .lower .upper ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 artist:Alok 64.3 60.2 68.3 ## 2 artist:Atlas_Genius 47.0 38.9 55.3 ## 3 artist:Au/Ra 59.5 52.1 67.0 ## 4 artist:Beyonc√© 69.1 65.7 72.6 ggplot(artist_summary_scaled, aes(x = artist, y = mu_j, ymin = .lower, ymax = .upper)) + geom_pointrange() + xaxis_text(angle = 90, hjust = 1) 16.4.5.1 QUiz ! Similar posterior mean but different 80%CI ? "],["posterior-prediction-1.html", "16.5 Posterior prediction", " 16.5 Posterior prediction What will be the popularity of new song of artist j (two cases: artist in the data / unknown artist)? posterior_predict() exist but first we do it by ‚Äúhand‚Äù! 16.5.1 First case: Frank Ocean (j=39) \\[Y^{i}_{new,j} | \\mu_j, \\sigma_y \\sim N(\\mu_j^{i}, (\\sigma^{(i)}_y)^2)\\] We have plenty of \\(\\mu^{i}_j\\) and \\(\\sigma^{(i)}_y\\) with two sources of variability : Not all song of Ocean are eqully popular (within-group sampling variability) we do not know the exact mean and variability of Ocean song (posterior variability) # Simulate Ocean&#39;s posterior predictive model set.seed(84735) ocean_chains &lt;- spotify_hierarchical_df |&gt; rename(b = `b[(Intercept) artist:Frank_Ocean]`) |&gt; select(`(Intercept)`, b, sigma) |&gt; mutate(mu_ocean = `(Intercept)` + b, y_ocean = rnorm(20000, mean = mu_ocean, sd = sigma)) # stuff that I always forget # Check it out head(ocean_chains, 3) ## (Intercept) b sigma mu_ocean y_ocean ## 1 45.73260 24.56671 14.11825 70.29930 79.71946 ## 2 45.96217 24.45931 13.14185 70.42148 68.79399 ## 3 47.42739 20.86424 15.57204 68.29163 80.45134 Then you summarize it: ocean_chains |&gt; mean_qi(y_ocean, .width = 0.80) ## # A tibble: 1 √ó 6 ## y_ocean .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 69.3 51.2 87.5 0.8 mean qi # to put into context # the range of a new song is wider tham the average of ocean artist_summary_scaled |&gt; filter(artist == &quot;artist:Frank_Ocean&quot;) ## # A tibble: 1 √ó 7 ## artist mu_j .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 artist:Frank_Ocean 69.4 66.6 72.2 0.8 mean qi 16.5.2 Posterior prediction for an observed group We do not have \\(\\mu_j\\) but we know new artist is an artist! And we now the range of mean popularity level among artist \\(N(\\mu, \\sigma_u)\\) and we have 44 artists. step 1: simulate \\(\\mu_{new_artist}\\) bt drawing into layer 2 of MCMC step 2: simulate song popularity with Layer 1 and \\(\\mu_{new_artist}\\) We are adding a new source of variability (not all artist are equally popular : between group) set.seed(84735) mohsen_chains &lt;- spotify_hierarchical_df |&gt; mutate(sigma_mu = sqrt(`Sigma[artist:(Intercept),(Intercept)]`), mu_mohsen = rnorm(20000, `(Intercept)`, sigma_mu), # new stuff y_mohsen = rnorm(20000, mu_mohsen, sigma)) # Posterior predictive summaries mohsen_chains |&gt; mean_qi(y_mohsen, .width = 0.80) ## # A tibble: 1 √ó 6 ## y_mohsen .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 52.3 25.9 78.6 0.8 mean qi 16.5.3 posterior_predict() set.seed(84735) prediction_shortcut &lt;- posterior_predict( spotify_hierarchical, newdata = data.frame(artist = c(&quot;Frank Ocean&quot;, &quot;Mohsen Beats&quot;))) # Posterior predictive model plots mcmc_areas(prediction_shortcut, prob = 0.8) + ggplot2::scale_y_discrete(labels = c(&quot;Frank Ocean&quot;, &quot;Mohsen Beats&quot;)) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. "],["shrinkage-bias_variance-trade-off.html", "16.6 Shrinkage &amp; bias_variance trade-off", " 16.6 Shrinkage &amp; bias_variance trade-off set.seed(84735) predictions_hierarchical &lt;- posterior_predict(spotify_hierarchical, newdata = artist_means) # Posterior predictive plots ppc_intervals(artist_means$popularity, yrep = predictions_hierarchical, prob_outer = 0.80) + ggplot2::scale_x_continuous(labels = artist_means$artist, breaks = 1:nrow(artist_means)) + xaxis_text(angle = 90, hjust = 1) + geom_hline(yintercept = 58.4, linetype = &quot;dashed&quot;) Quizz What is shringage in this example ? Shrinkage refers to the phenomenon in which the group-specific local trends in a hierarchical model are pulled or shrunk toward the global trends. Shrinkage increases as the number of observations on group j, nj, decreases. That is, we rely more and more on global trends to understand a group for which we have little data. Shrinkage increases when the variability within groups, œÉy, is large in comparison to the variability between groups, œÉŒº. That is, we rely more and more on global trends to understand a group when there is little distinction in the patterns from one group to the next The artists that shrunk the most are those with smaller sample sizes nj and popularity levels at the extremes of the spectrum. 16.6.1 Quizzz! With no pooled, complete pooled and hierarchical: Same population, other sample: which would be the most/least variable? Most biased/least estinating artist mean popularity levels? "],["not-everything-is-hierarchical.html", "16.7 Not everything is hierarchical", " 16.7 Not everything is hierarchical Distinction between a predictor and a grouping variable can only be made if we understand how data was collected. "],["summary-4.html", "16.8 Summary", " 16.8 Summary First model with groups! observations on one group are independent to another group but correlated in the same group New parameters : group-specific global parameters learning for one group to another will lead to some shrinkage this models are less variable than no pooling and less biased than complete pooling "],["meeting-videos-15.html", "16.9 Meeting Videos", " 16.9 Meeting Videos 16.9.1 Cohort 1 Meeting chat log LOG 16.9.2 Cohort 2 Meeting chat log LOG "],["normal-hierarchical-models-with-predictors.html", "Chapter 17 (Normal) Hierarchical Models with Predictors", " Chapter 17 (Normal) Hierarchical Models with Predictors Learning objectives: Build hierarchical (H) regression models of response variable \\(Y\\) by predictors \\(X\\) Evaluate and compare H and non H models Use H models for posterior prediction 17.0.1 Data set We are returning on a subset of the Cherry Blossom 10 mile running race analysis # Load packages library(bayesrules) library(tidyverse) library(rstanarm) library(bayesplot) library(tidybayes) library(broom.mixed) # Load data data(cherry_blossom_sample) running &lt;- cherry_blossom_sample A bit of data wrangling: running &lt;- running |&gt; select(runner, age, net) |&gt; na.omit() nrow(running) ## [1] 185 unique(running$runner) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 ## 36 Levels: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ... 36 We have 36 runners and 185 rows. "],["quick-complete-pooling-option.html", "17.1 Quick: complete pooling option", " 17.1 Quick: complete pooling option \\[Y_{ij} | \\beta_0, \\beta_1, \\sigma \\sim N(\\mu_i, \\sigma^2)\\] \\(Y_{ij}\\) running time with \\(j\\) runner and \\(i\\) race \\[\\mu_i = \\beta_0 + \\beta_1X_{ij}\\] \\(X_{ij}\\) Age Then we have global parameters (also here priors) \\[\\beta_{0c} \\sim N (0, 35^2)\\] This is the intercept centered \\[\\beta_1 \\sim N(0, 15^2)\\] \\[\\sigma \\sim Exp(0,072)\\] If we go with this model: no relationship between age and running time. complete_pooled_model &lt;- stan_glm( net ~ age, data = running, family = gaussian, prior_intercept = normal(0, 2.5, autoscale = TRUE), prior = normal(0, 2.5, autoscale = TRUE), prior_aux = exponential(1, autoscale = TRUE), chains = 4, iter = 5000*2, seed = 84735) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.7e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 1: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 1: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 1: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 1: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 1: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 1: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 1: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 1: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 1: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 1: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 1: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.118959 seconds (Warm-up) ## Chain 1: 0.225793 seconds (Sampling) ## Chain 1: 0.344752 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.6e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 2: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 2: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 2: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 2: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 2: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 2: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 2: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 2: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 2: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 2: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 2: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.123383 seconds (Warm-up) ## Chain 2: 0.221596 seconds (Sampling) ## Chain 2: 0.344979 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.4e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 3: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 3: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 3: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 3: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 3: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 3: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 3: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 3: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 3: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 3: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 3: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.123367 seconds (Warm-up) ## Chain 3: 0.226533 seconds (Sampling) ## Chain 3: 0.3499 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.7e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 4: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 4: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 4: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 4: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 4: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 4: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 4: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 4: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 4: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 4: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 4: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.119643 seconds (Warm-up) ## Chain 4: 0.213973 seconds (Sampling) ## Chain 4: 0.333616 seconds (Total) ## Chain 4: "],["hierarchical-model-with-varying-intercept.html", "17.2 Hierarchical Model with varying intercept", " 17.2 Hierarchical Model with varying intercept 17.2.1 Model buildings 17.2.1.1 Layer 1 Within-group: within runner \\[Y_{ij} | \\beta_0, \\beta_1, \\sigma_j \\sim N(\\mu_{ij}, \\sigma_j^2)\\] We added a bunch of \\(j\\)! So now we have runner specific mean (\\(\\mu_{ij}\\)) and the variance with a runner (\\(\\sigma_j\\)) \\[\\mu_{ij} = \\beta_{0j} + \\beta_1X_{ij}\\] Here we are using a specific intercept for each runner (\\(\\beta_{oj}\\)) but we are still using a global age coefficient (\\(\\beta_1\\)). 17.2.1.2 Layer 2: Between Runners Quizz! Which of our current parameters (\\(\\beta_{0j}, \\beta_1, \\sigma_y\\)) do we need to model in the next layer? (hint:title) \\[\\beta_{0j} | \\beta_{0}, \\sigma_0 \\overset{\\text{ind}}{\\sim} N(\\beta_0, \\sigma_0^2)\\] \\(\\beta_{0j}\\) is our intercept for each runner and it follow a normal distribution with the global average of intercept (\\(\\beta_0\\)) and the between-group variability (\\(\\sigma_0\\)). Now quiz! For Which model parameters must we specify priors in the final layer of our hierarchical regression model? \\[\\beta_{0c} \\sim N(m_0, s_0^2)\\] \\[\\beta_1 \\sim N(m_1, s_1^2)\\] \\[\\sigma_y \\sim Exp(l_y)\\] \\[\\sigma_0 \\sim Exp(l_0)\\] Normal hierarchical regression assumptions: structure of the data: conditioned on \\(X_{ij}\\), \\(Y_{ij}\\) on any group j is independant of other group k but different data point within the same group are correlated structure of the relationship: Linear relation structure of variability within groups: Within any group j at any predictor value \\(X_{ij}\\) the observed values of \\(Y_{ij}\\) will vary normally Structure of variability between groups 17.2.1.3 Tuning the prior \\[\\beta_{0c} \\sim N(100, 10^2)\\] runing tine is around 80 - 120 mins \\[\\beta_1 \\sim N(2.5, 1^2)\\] We just know that it increase and it can range from 0.5 to 4.5 mins / year (on average) \\[\\sigma_y \\sim Exp(0.078)\\] \\[\\sigma_0 \\sim Exp(1)\\] Then we use weakly informative priors. running_model_1_prior &lt;- stan_glmer( net ~ age + (1 | runner), # formula data = running, family = gaussian, prior_intercept = normal(100, 10), prior = normal(2.5, 1), prior_aux = exponential(1, autoscale = TRUE), prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1), chains = 4, iter = 5000*2, seed = 84735, prior_PD = TRUE) # just the prior ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.8e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 1: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 1: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 1: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 1: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 1: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 1: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 1: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 1: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 1: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 1: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 1: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.46389 seconds (Warm-up) ## Chain 1: 0.48641 seconds (Sampling) ## Chain 1: 0.9503 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 2.4e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 2: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 2: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 2: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 2: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 2: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 2: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 2: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 2: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 2: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 2: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 2: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.463261 seconds (Warm-up) ## Chain 2: 0.491682 seconds (Sampling) ## Chain 2: 0.954943 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.9e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 3: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 3: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 3: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 3: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 3: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 3: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 3: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 3: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 3: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 3: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 3: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.468224 seconds (Warm-up) ## Chain 3: 0.496109 seconds (Sampling) ## Chain 3: 0.964333 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.8e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 4: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 4: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 4: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 4: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 4: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 4: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 4: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 4: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 4: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 4: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 4: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.463297 seconds (Warm-up) ## Chain 4: 0.487417 seconds (Sampling) ## Chain 4: 0.950714 seconds (Total) ## Chain 4: running |&gt; # here we just used 100 sims add_predicted_draws(running_model_1_prior, n = 100) |&gt; ggplot(aes(x = net)) + geom_density(aes(x = .prediction, group = .draw)) + xlim(-100,300) ## Warning: ## In add_predicted_draws(): The `n` argument is a deprecated alias for `ndraws`. ## Use the `ndraws` argument instead. ## See help(&quot;tidybayes-deprecated&quot;). ## Warning: Removed 46 rows containing non-finite values (stat_density). "],["posterior-simulation-and-analysis.html", "17.3 Posterior simulation and analysis", " 17.3 Posterior simulation and analysis # Simulate the posterior !!! new command you can set/update running_model_1 &lt;- update(running_model_1_prior, prior_PD = FALSE) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 6.6e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.66 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 1: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 1: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 1: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 1: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 1: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 1: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 1: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 1: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 1: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 1: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 1: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 3.6212 seconds (Warm-up) ## Chain 1: 3.42691 seconds (Sampling) ## Chain 1: 7.04812 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 3.7e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 2: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 2: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 2: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 2: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 2: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 2: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 2: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 2: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 2: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 2: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 2: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 3.70438 seconds (Warm-up) ## Chain 2: 3.41696 seconds (Sampling) ## Chain 2: 7.12134 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 4e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 3: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 3: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 3: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 3: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 3: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 3: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 3: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 3: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 3: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 3: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 3: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 3.65162 seconds (Warm-up) ## Chain 3: 3.47896 seconds (Sampling) ## Chain 3: 7.13057 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3.8e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 4: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 4: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 4: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 4: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 4: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 4: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 4: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 4: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 4: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 4: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 4: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 3.42673 seconds (Warm-up) ## Chain 4: 3.46438 seconds (Sampling) ## Chain 4: 6.89111 seconds (Total) ## Chain 4: # Check the prior specifications prior_summary(running_model_1) ## Priors for model &#39;running_model_1&#39; ## ------ ## Intercept (after predictors centered) ## ~ normal(location = 100, scale = 10) ## ## Coefficients ## ~ normal(location = 2.5, scale = 1) ## ## Auxiliary (sigma) ## Specified prior: ## ~ exponential(rate = 1) ## Adjusted prior: ## ~ exponential(rate = 0.072) ## ## Covariance ## ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details # Markov chain diagnostics # mcmc_trace(running_model_1) # mcmc_dens_overlay(running_model_1) # mcmc_acf(running_model_1) # neff_ratio(running_model_1) # rhat(running_model_1) Data output and model: (Intercept) = \\(\\beta_0\\) age - \\(\\beta_1\\) b[(intercept) runner:j] = \\(b_{0j} = \\beta_{0j} - \\beta_0\\) sigma = \\(\\sigma_y\\) Sigma[runner:(Intercept), (Intercept)] = \\(\\sigma_0^2\\) 17.3.0.1 Posterior analysis of the global relationship \\[\\beta_0 + \\beta_1X\\] tidy_summary_1 &lt;- tidy(running_model_1, effects = &quot;fixed&quot;, conf.int = TRUE, conf.level = 0.80) tidy_summary_1 ## # A tibble: 2 √ó 5 ## term estimate std.error conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 19.3 12.0 3.78 34.9 ## 2 age 1.30 0.216 1.02 1.58 So runners are slowing down with age! 17.3.0.2 Posterior analysis of group-specific relationships \\[\\beta_{0j} + \\beta_1X_{ij} = (\\beta_0 + b_{0j}) + \\beta_1X_{ij} \\] # Posterior summaries of runner-specific intercepts # we go from wide to long runner_summaries_1 &lt;- running_model_1 |&gt; spread_draws(`(Intercept)`, b[,runner]) |&gt; mutate(runner_intercept = `(Intercept)` + b) |&gt; select(-`(Intercept)`, -b) |&gt; median_qi(.width = 0.80) |&gt; select(runner, runner_intercept, .lower, .upper) runner_summaries_1 ## # A tibble: 36 √ó 4 ## runner runner_intercept .lower .upper ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 runner:1 5.54 -10.1 21.0 ## 2 runner:10 43.9 27.9 59.9 ## 3 runner:11 19.4 4.02 34.7 ## 4 runner:12 1.09 -14.7 16.7 ## 5 runner:13 11.1 -4.41 26.9 ## 6 runner:14 24.0 8.14 39.7 ## 7 runner:15 25.8 10.4 41.1 ## 8 runner:16 23.6 8.00 38.8 ## 9 runner:17 29.5 14.1 44.7 ## 10 runner:18 31.5 15.9 46.8 ## # ‚Ä¶ with 26 more rows running |&gt; filter(runner %in% c(&quot;4&quot;, &quot;5&quot;)) |&gt; add_fitted_draws(running_model_1, n = 100) |&gt; ggplot(aes(x = age, y = net)) + geom_line( aes(y = .value, group = paste(runner, .draw), color = runner), alpha = 0.1) + geom_point(aes(color = runner)) ## Warning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing. ## Use [add_]epred_draws() to get the expectation of the posterior predictive. ## Use [add_]linpred_draws() to get the distribution of the linear predictor. ## For example, you used [add_]fitted_draws(..., scale = &quot;response&quot;), which ## means you most likely want [add_]epred_draws(...). 17.3.0.3 Posterior analysis of within- and between group variability tidy_sigma &lt;- tidy(running_model_1, effects = &quot;ran_pars&quot;) tidy_sigma ## # A tibble: 2 √ó 3 ## term group estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sd_(Intercept).runner runner 13.3 ## 2 sd_Observation.Residual Residual 5.25 sigma_0 &lt;- tidy_sigma[1,3] sigma_y &lt;- tidy_sigma[2,3] sigma_0^2 / (sigma_0^2 + sigma_y^2) # between ## estimate ## 1 0.8653185 sigma_y^2 / (sigma_0^2 + sigma_y^2) # within ## estimate ## 1 0.1346815 "],["hierarchical-model-with-varying-intercepts-slopes.html", "17.4 Hierarchical model with varying intercepts &amp; slopes", " 17.4 Hierarchical model with varying intercepts &amp; slopes ggplot(running, aes(x = age, y = net, group = runner)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.5) ## `geom_smooth()` using formula &#39;y ~ x&#39; Quiz! source:thebrain187 How can we modify our random intercepts models to recognize that the rate at which running time change with age might vary from runner to runner? 17.4.1 Model building \\[Y_{ij} | \\beta_{0j}, \\beta_{1j}, \\sigma_y \\sim N(\\mu_{ij}, \\sigma_y^2)\\] \\[\\mu_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij}\\] \\[\\beta_{0j} | \\beta_{0}, \\sigma_0 \\sim N(\\beta_0, \\sigma_0^2)\\] \\[\\beta_{1j} | \\beta_{1}, \\sigma_1 \\sim N(\\beta_1, \\sigma_1^2)\\] But \\(\\beta_{0j}\\) and \\(\\beta_{1j}\\) are correlated for runner j. Let \\(\\rho \\in [-1,1]\\) represent the correlation between \\(\\beta_{0j}\\) and \\(\\beta_(1j)\\). We will need to do a joint Normal model of both: \\[\\begin{pmatrix} \\beta_{0j} \\\\ \\beta_{1j} \\end{pmatrix} | \\beta_0, \\beta_1, \\sigma_0, \\sigma_1 \\sim N \\begin{pmatrix}\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}, \\Sigma \\end{pmatrix}\\] \\[ \\Sigma = \\begin{pmatrix} \\sigma_0¬≤ &amp; \\rho\\sigma_0\\sigma_1 \\\\ \\rho\\sigma_0\\sigma_1 &amp; \\sigma_1^2 \\end{pmatrix} \\] \\(\\Sigma\\) is our covariance matrix Let me google it for you: \\[\\rho(X,Y) = \\frac{Cov(X,Y)}{\\sigma_X\\sigma_y} \\in [-1, 1]\\] Examples: strong negative correlation between \\(\\beta_{0j}\\) and \\(\\beta_{1j}\\) with small intercept: smaller you start higher you go strong positive correlation with small intercept : smaller you start lower you go no correlation : X and Y do their life Quiz! \\(\\beta_{0j}\\) and \\(\\beta_{1j}\\) are negatively correlated: Runners that start out slower (i.e., with a higher baseline), also tend to slow down at a more rapid rate. The rate at which runners slow down over time isn‚Äôt associated with how fast they start out. Runners that start out faster (i.e., with a lower baseline), tend to slow down at a more rapid rate. \\(\\beta_{0j}\\) and \\(\\beta_{1j}\\) are positively correlated: If \\(\\sigma_1 = 0\\), age will not differ group to group we are back to the random intercepts model. But how do we get our Joint prior model? We are using a decomposition of covariance model and the function decov() (rememver the prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1) in our stan_glmer call) We can decompose our matrix in 3 components: \\[R = \\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix}\\] \\[ \\tau = \\sqrt{\\sigma^2_0 + \\sigma^2_1}\\] \\[ \\pi = \\begin{pmatrix} \\pi_0 \\\\ \\pi_1 \\end{pmatrix} = \\begin{pmatrix}\\frac{\\sigma_0^2}{\\sigma_0^2 + \\sigma_1^2} \\\\ \\frac{\\sigma_1^2}{\\sigma_0^2 + \\sigma_1^2} \\end{pmatrix}\\] \\[R \\sim LKJ(\\eta)\\] Lewandowski-Kurowicka-Joe (LKJ) distribution with \\(\\eta\\) as **regularization hyperparameter if \\(\\eta &lt; 1\\) prior with strong correlation unsure of postive or negative if \\(\\eta = 1\\) flat prior between - 1 and 1 if \\(\\eta &gt; 1\\) prior indicating low correlation For \\(\\tau\\) we can use a Gamma prior (or here the exponential special case). It use two parameters : shape and scale. Finaly for \\(\\pi\\) we know that the sum of them will be one (remember they are the relative proportion of the variability between group). This means we will be able to use a symmetric Dirichlet(\\(2, \\delta\\)). \\(\\delta\\) is called a concentration hyperparameter. In this case with two group it can be define as a Beta distribution with both (\\(\\delta\\)). if \\(\\delta &lt; 1\\) more prior on \\(\\pi_0\\) on 0, 1 -&gt; either a lot of few of the variability between group is explained with intercept \\(\\delta = 1\\) flat prior on \\(\\pi_0\\) variability of the intercept can explain from 0 to all the variability between groups \\(\\delta &gt; 1\\) our prior is that around half of the variability between group is explained by differences in intercepts and rest with slopes. To sum it up when we use rstanarm decov(): reg = 1 is for \\(R \\sim LKJ(1)\\) shape = 1 , scale = 1 is for \\(\\tau \\sim Gamma(1,1)\\) or \\(Exp(1)\\) conc = 1 is for \\(Dirichlet(2,1)\\) (two parameters with \\(\\delta = 1\\)) 17.4.2 Posterior simulation and anlysis running_model_2 &lt;- stan_glmer( net ~ age + (age | runner), data = running, family = gaussian, prior_intercept = normal(100, 10), prior = normal(2.5, 1), prior_aux = exponential(1, autoscale = TRUE), prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1), chains = 4, iter = 5000*2, seed = 84735, adapt_delta = 0.99999 # change here ) Now we have 78 parameters (36 for intercepts / 36 age coefficient and 6 global parameters) ! My poor laptop have done it in 33 minutes! 17.4.2.1 Global / Group specific parameters : \\[\\beta_0 + \\beta_1 X \\] # Quick summary of global regression parameters tidy(running_model_2, effects = &quot;fixed&quot;, conf.int = TRUE, conf.level = 0.80) # A tibble: 2 x 5 term estimate std.error conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 18.5 11.6 3.61 33.6 2 age 1.32 0.217 1.04 1.59 We need to move the MCMC simulations result into a friendlier objet: # str(running_model_2) try it! runner_chains_2 &lt;- running_model_2 |&gt; spread_draws(`(Intercept)`, b[term, runner], `age`) |&gt; pivot_wider(names_from = term, names_glue = &quot;b_{term}&quot;, values_from = b) |&gt; mutate(runner_intercept = `(Intercept)` + `b_(Intercept)`, runner_age = age + b_age) dim(runner_chains_2) We need to summarize a bit: runner_summaries_2 &lt;- runner_chains_2 |&gt; group_by(runner) |&gt; summarize(runner_intercept = median(runner_intercept), runner_age = median(runner_age)) # Check it out head(runner_summaries_2, 3) saveRDS(runner_summaries_2, &quot;data/ch17/runner_summaries_2&quot;) runner_summaries_2 &lt;- readRDS(&quot;data/ch17/runner_summaries_2&quot;) ggplot(running, aes(y = net, x = age, group = runner)) + geom_abline(data = runner_summaries_2, color = &quot;gray&quot;, aes(intercept = runner_intercept, slope = runner_age)) + lims(x = c(50, 61), y = c(50, 135)) They slopes differ but no so much -&gt; shrinkage the model is still trying to balance between a complete pooled models and a no pooled one (see fig 17.16). 17.4.2.2 Within- and between-group variability is it worth it ? tidy(running_model_2, effects = &quot;ran_pars&quot;) # A tibble: 4 x 3 term group estimate &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 sd_(Intercept).runner runner 1.34 2 sd_age.runner runner 0.251 3 cor_(Intercept).age.runner runner -0.0955 4 sd_Observation.Residual Residual 5.17 We had 5.25 as \\(\\sigma_y\\) before, We have very slight correlation between \\(\\beta_0j\\) and \\(\\beta_1j\\). "],["model-evaluation-and-selection.html", "17.5 Model evaluation and selection", " 17.5 Model evaluation and selection How fair is each model? How wrong is each model? How accurate are each model posterior prediction? For 2: pp_check(complete_pooled_model) + labs(x = &quot;net&quot;, title = &quot;complete pooled model&quot;) pp_check(running_model_1) + labs(x = &quot;net&quot;, title = &quot;running model 1&quot;) # Not displaying because MCMC of running_model_2 is to slow # pp_check(running_model_2) + # labs(x = &quot;net&quot;, title = &quot;running model 2&quot;) We can drop the complete pooled model. # Calculate prediction summaries set.seed(84735) prediction_summary(model = running_model_1, data = running) mae mae_scaled within_50 within_95 1 2.626 0.456 0.6865 0.973 prediction_summary(model = running_model_2, data = running) mae mae_scaled within_50 within_95 1 2.53 0.4424 0.7027 0.973 they are very close! But what about ‚Äúunknown data‚Äù? We will use CV but here we divide runners. (I did not run it as I was afraid of computation time!) Using expected log-predictive densities (ELPD) we do not find significant difference in posterior accuracy for the two models. Is the additional complexity worth it? Here no. "],["posterior-prediction-2.html", "17.6 Posterior prediction", " 17.6 Posterior prediction We will use running_model_1. We will try to predict for runner1, runner10 and Miles (one of the authors) when they will be 61 years old. running |&gt; filter(runner %in% c(&quot;1&quot;, &quot;10&quot;)) |&gt; ggplot(data = _ , aes(x = age, y = net)) + geom_point() + facet_grid(~ runner) + lims(x = c(54, 61)) ## Warning: Removed 1 rows containing missing values (geom_point). We will have two sources of uncertainty in runner 1 and 10 (within-group sampling variability \\(\\sigma_y\\), posterior variability, \\(\\beta_{0j}\\), \\(\\beta_1\\) and \\(\\sigma_y\\)) and for Miles we need to add the between-group sampling variability (\\(\\sigma_0\\)). set.seed(84735) predict_next_race &lt;- posterior_predict( running_model_1, newdata = data.frame(runner = c(&quot;1&quot;, &quot;Miles&quot;, &quot;10&quot;), age = c(61, 61, 61))) apply(predict_next_race, 2, median) ## 1 2 3 ## 84.57572 98.12049 122.99916 mcmc_areas(predict_next_race, prob = 0.8) + ggplot2::scale_y_discrete(labels = c(&quot;runner 1&quot;, &quot;Miles&quot;, &quot;runner 10&quot;)) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. "],["details-longitudinal-data.html", "17.7 Details: Longitudinal data", " 17.7 Details: Longitudinal data We observe each runner over time are are interest in effect of time: age is longitudinal. We are making the assumptions that our correlation will be the same across all ages we do not take into account that age close to each other tend to be more correlated. It is possible to add that into our model for that see bayeslongitudinal R package. "],["example-danceability.html", "17.8 Example Danceability", " 17.8 Example Danceability Next week ? "],["chapter-summary-1.html", "17.9 Chapter summary", " 17.9 Chapter summary \\(Y_{ij}|\\beta_j, \\sigma_y \\sim N(\\mu_{ij}, \\sigma¬≤_y)\\) : regression model within group \\(j\\) \\(\\beta_j|\\beta, \\sigma \\sim N(\\beta, \\sigma^2)\\) : variability in regression parameters between group \\(\\beta, \\sigma_y, \\sigma, ... \\sim ...\\) priors models on global parameters Either we go with a random intercepts models or we use a random intercepts and slopes model. "],["meeting-videos-16.html", "17.10 Meeting Videos", " 17.10 Meeting Videos 17.10.1 Cohort 1 Meeting chat log LOG 17.10.2 Cohort 2 Meeting chat log LOG "],["non-normal-hierarchical-regression-classification.html", "Chapter 18 Non-Normal Hierarchical Regression &amp; Classification", " Chapter 18 Non-Normal Hierarchical Regression &amp; Classification Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-9.html", "18.1 SLIDE 1", " 18.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-17.html", "18.2 Meeting Videos", " 18.2 Meeting Videos 18.2.1 Cohort 1 Meeting chat log LOG 18.2.2 Cohort 2 Meeting chat log LOG "],["adding-more-layers.html", "Chapter 19 Adding More Layers", " Chapter 19 Adding More Layers Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-10.html", "19.1 SLIDE 1", " 19.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-18.html", "19.2 Meeting Videos", " 19.2 Meeting Videos 19.2.1 Cohort 1 Meeting chat log LOG 19.2.2 Cohort 2 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
