# Hierarchical Models are Exciting

**Welcome to Unit 4 - The final *unit* !**

Hierarchical models expand our toolbox for hierarchical (grouped / pooled) data:

 - a sampled group of schools and data $y$ on multiple individual students within each school
 
 - a sampled group of labs and data $y$ from multiple individual experiments within each lab

 - a sampled group of people on whom we make multiple individual observations of information $y$ over time.
 
Unit four will explore these techniques in these final chapters:

- Chapter 15: Introduce the concepts

- Chapter 16: Hierarchical models without predictors

- Chapter 17: Normal Hierarchical Models with predictors

- Chapter 18: Non-Normal Hierarchical Regression

- Chapter 19: More Layers!

## Hierarchical Models are Exciting

**Learning objectives**

- Explore the limitations of our current Bayesian modeling toolbox under two extremes, complete pooling and no pooling.

- Examine the benefits of the partial pooling provided by hierarchical Bayesian models.

- Focus on the big ideas and leave the details to subsequent chapters.

```{r, message = FALSE, warning = FALSE}
# Load packages
library("broom.mixed")
library("bayesrules")
library("dplyr")
library("ggplot2")
library("rstanarm")

sessionInfo()
```


## Data: Cherry Blossom 5K

![Cherry Blossom 5K and 10-mile races](images/cherry_blossom_5K.png)

```{r}
# Load data
data(cherry_blossom_sample)
running <- cherry_blossom_sample %>% 
  select(runner, age, net)
```

```{r}
table(running$runner)
```

```{r}
summary(running$age, give.attr = FALSE)
```

```{r}
summary(running$net, give.attr = FALSE)
```


## Pooled / Grouped data

- Why do we need hierachical models ? 

- We will use the `cherry_blossom_sample` data:

```{r, message=FALSE, echo=FALSE, warning=FALSE}
ggplot(running, aes(x = runner, y = net)) + 
  geom_boxplot()
```

- Many runners ran multiple races as they aged 

**GOAL: understand relationship between running time and age.**

## Complete Pooling

- Complete pooling just combines all the observations, ignoring which runner they came from



```{r, message=FALSE, echo=FALSE, warning=FALSE}
ggplot(running, aes(y = net, x = age)) + 
  geom_point()
```



- No clear trend appears, book performs linear regression and finds slope consistent with zero.

- This seems strange: dont we get slower as we age?

- Zoom in on three runners:

```{r, message=FALSE,warning=FALSE}
# Select an example subset
examples <- running %>% 
  filter(runner %in% c("1", "20", "22"))

ggplot(examples, aes(x = age, y = net)) + 
  geom_point() + 
  facet_wrap(~ runner) + 
  geom_abline(aes(intercept = 75.2242, slope = 0.2678), 
              color = "blue")
```



### Drawbacks of complete pooling

- Violates assumption of independence! Observations within a runner are correlated.

- Ignores information about individual runners: people age differently!

- Produce misleading conclusions of the relationship between predictor and response

## No pooling

* Treat each runner separately, with a separate fit for each.

* Multiple fits, deal with each of the 36 runners *seperately*

Example for 3 runners:

```{r, message=FALSE, echo=FALSE, warning=FALSE}
 


ggplot(examples, aes(x = age, y = net)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) + 
  facet_wrap(~ runner) + 
  xlim(52, 62)
```

* Seems great at first glance....

* However some significant drawbacks:
 
    - Cannot reliably generalize to groups outside our sample. 
    
    - Assumes one group doesnt contain any information about another, ignores potentially useful information! (For example, do we really think that first runner gets faster with age?)
    
## Hierarchical data



* Cherry Blossom data presents a new challenge: Hierachical data. 

`r knitr::include_graphics(rep("images/partial_pool_diagram.png"))`

* Hierarchical structure - each group is unique but also connected. Common structure:

    * Groups could be schools and observations students within each school
    
    * Groups could be labs, observations multiple experiments in each lab
    
    * Groups could be subjects, observations are a series of tests (Exercise 15.3ff)
    
* Middle ground between complete and no pooling: Partial pooling.

* Provides insight into both

    1. Within-group variability  
    
    2. Between-group variability 
    
 

## Partial pooling

 

* Example results (we will see *how* to do this next week)

```{r, echo=FALSE,  out.width = "100%", fig.cap= "Posterior median models of the no pooled (blue), complete pooled (black), and hierarchical (dashed) models of running time vs age for three example runners and you."}

knitr::include_graphics(rep("images/runners-ch-15-1.png"))

```


* Note the fit no longer predicts the first runner getting better with age..  information from other runners informed this fit.

* Also note that partial pooling allows us to make predictions for new groups... like YOU! 

## Summary

* We (I hope) motivated the need for hierarchical models. 

* Explored briefly complete pooling, no pooling and partial pooling models.



## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/eT7x7ujjhxg")`

<details>
<summary> Meeting chat log </summary>

```
00:29:42	Brendan Lam:	Hes got a point
00:31:21	Will Parbury:	https://en.wikipedia.org/wiki/Shrinkage_(statistics)
00:31:56	Brendan Lam:	http://haines-lab.com/post/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/
00:32:03	Brendan Lam:	‚ÄúOn the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression"
00:33:45	defuneste:	https://docs.google.com/spreadsheets/d/18IDSOU2bfkD55kOB18qCB7Idbpiyp4_9qeWjkvE-Syc/edit#gid=0
00:37:51	Erik Aa:	https://www.youtube.com/watch?v=SocRgsf202M
00:38:02	Lisa Lau:	‚Äúhierarchical Bayesian models actually contain frequentist ridge and LASSO regression as a special case‚Äînamely, we can choose a prior distribution across the Œ≤ weights that gives us a solution that is equivalent to that of the frequentist ridge or LASSO methods! Not only that, but Bayesian regression gives us a full posterior distribution for each parameter, thus circumventing problems with frequentist regularization that require the use of bootstrapping to estimate confidence intervals.‚Äù üòØ
00:38:17	Erik Aa:	wow
00:39:20	Brendan Lam:	Good luck!
```
</details>

### Cohort 2

`r knitr::include_url("https://www.youtube.com/embed/AydrtOASCUw")`


### Cohort 4

`r knitr::include_url("https://www.youtube.com/embed/URL")`

<details>
<summary> Meeting chat log </summary>

```
LOG
```
</details>
